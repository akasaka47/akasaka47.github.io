<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://akasaka47.github.io</id>
    <title>Leniakea</title>
    <updated>2022-07-20T00:06:53.126Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://akasaka47.github.io"/>
    <link rel="self" href="https://akasaka47.github.io/atom.xml"/>
    <subtitle>再不跑就来不及了</subtitle>
    <logo>https://akasaka47.github.io/images/avatar.png</logo>
    <icon>https://akasaka47.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, Leniakea</rights>
    <entry>
        <title type="html"><![CDATA[2s-AGCN测试结果]]></title>
        <id>https://akasaka47.github.io/post/jie-guo/</id>
        <link href="https://akasaka47.github.io/post/jie-guo/">
        </link>
        <updated>2022-07-19T14:15:59.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C">测试结果</a>
<ul>
<li><a href="#a%E8%AE%AD%E7%BB%83a%E6%B5%8B%E8%AF%95">A训练，A测试</a></li>
<li><a href="#b%E8%AE%AD%E7%BB%83b%E6%B5%8B%E8%AF%95">B训练，B测试</a></li>
<li><a href="#a%E8%AE%AD%E7%BB%83b%E6%B5%8B%E8%AF%95">A训练，B测试</a>
<ul>
<li><a href="#%E5%85%B3%E8%8A%82%E6%B5%81">关节流</a></li>
<li><a href="#%E9%AA%A8%E9%AA%BC%E6%B5%81">骨骼流</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h1 id="测试结果">测试结果</h1>
<h2 id="a训练a测试">A训练，A测试</h2>
<p>训练集-A：200000<br>
测试集-A：65508</p>
<p>35个epoch训练结果</p>
<pre><code class="language-py"># 关节流：
weights:         10_class/PF_agcn_joint-35-28116.pt
precision:       0.8443670150987224
recall:          0.7876489707475623
F1:              0.5053653275299987

# 骨骼流：
weights:         10_class/PF_agcn_bone-35-28116.pt
precision:       0.84192037470726
recall:          0.7789815817984832
F1:              0.5004692858577718

# 双流综合
# 综合评价 = (关节流 + 骨骼流 * alpha) / (1 + alpha)
alpha:           1
precision:       0.879356190823317
recall:          0.8183205753798478
F1:              0.8477411935697685
</code></pre>
<hr>
<h2 id="b训练b测试">B训练，B测试</h2>
<p>训练集-B：250000<br>
测试集-B：139081</p>
<p>29个epoch训练结果</p>
<pre><code class="language-py"># 关节流：
weights:         10_class/B/PF_agcn_joint-29-29280.pt
precision:       0.8430769230769231
recall:          0.6807453416149069
F1:              0.4548027775463997

# 骨骼流
weights:         10_class/B/PF_agcn_bone-29-29280.pt
precision:       0.8244897959183674
recall:          0.7527950310559006
F1:              0.48164782955115537

# 双流综合
# 综合评价 = (关节流 + 骨骼流 * alpha) / (1 + alpha)
alpha:           4.0
precision:       0.6650039871759402
recall:          0.5856128064509964
F1:              0.6227884564092534
</code></pre>
<p>35个epoch训练结果</p>
<pre><code class="language-py"># 关节流：
accuracy:        0.8830142813734427
precision:       0.8159879336349924
recall:          0.6720496894409937
F1:              0.44081683677196304


# 骨骼流
accuracy:        0.886052871467639
precision:       0.8410174880763116
recall:          0.6571428571428571
F1:              0.442460497845337

# 双流综合
# 综合评价 = (关节流 + 骨骼流 * alpha) / (1 + alpha)
alpha:           1.0
accuracy:        0.8713807062071742
precision:       0.6958622386689507
recall:          0.5619819234517417
F1:              0.6217972164139817
</code></pre>
<p>50个epoch训练结果</p>
<pre><code class="language-py"># 关节流：
accuracy:        0.8839258584017016
precision:       0.8196969696969697
recall:          0.6720496894409937
F1:              0.44216139863203574

# 骨骼流：
accuracy:        0.8629595867517472
precision:       0.7753510140405616
recall:          0.6173913043478261
F1:              0.40012246216996156

# 双流综合
# 综合评价 = (关节流 + 骨骼流 * alpha) / (1 + alpha)
alpha:           2.0
accuracy:        0.8662118785866818
precision:       0.6766971526118312
recall:          0.5531718582374956
F1:              0.6087312031966771
</code></pre>
<hr>
<h2 id="a训练b测试">A训练，B测试</h2>
<p>使用数据集-A的前200000条数据作为训练集<br>
数据集-B的全部389081条数据作为测试集</p>
<p>测试结果很差，<code>F1</code>一直很低，而且数据集-B上的<code>loss</code>在持续升高</p>
<p>训练过程如下：</p>
<h3 id="关节流">关节流</h3>
<pre><code>[ Tue Jul 19 19:13:44 2022 ]    Mean test loss of 760 batches: 0.6305880053655097.
[ Tue Jul 19 19:13:46 2022 ]    Top1: 4.22%
[ Tue Jul 19 19:13:54 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:13:54 2022 ] Training epoch: 21
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 19:17:22 2022 ]    Mean training loss: 0.2173.
[ Tue Jul 19 19:17:22 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:17:22 2022 ] Eval epoch: 21
100%|██████████████████████████████████████████████████████| 760/760 [01:50&lt;00:00,  6.88it/s]

accuracy:        0.7455083491862186
precision:       0.38724373576309795
recall:          0.15370705244122965
F1:              0.0772537236829443

[ Tue Jul 19 19:19:12 2022 ]    Mean test loss of 760 batches: 0.6623714366241505.
[ Tue Jul 19 19:19:14 2022 ]    Top1: 5.66%
[ Tue Jul 19 19:19:22 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:19:22 2022 ] Training epoch: 22
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.75it/s]
[ Tue Jul 19 19:22:50 2022 ]    Mean training loss: 0.2147.
[ Tue Jul 19 19:22:50 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:22:50 2022 ] Eval epoch: 22
100%|██████████████████████████████████████████████████████| 760/760 [01:49&lt;00:00,  6.91it/s]

accuracy:        0.7624180934263369
precision:       0.45701357466063347
recall:          0.09132007233273057
F1:              0.0539089398155121

[ Tue Jul 19 19:24:41 2022 ]    Mean test loss of 760 batches: 0.6442578231915832.
[ Tue Jul 19 19:24:43 2022 ]    Top1: 10.66%
[ Tue Jul 19 19:24:51 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:24:51 2022 ] Training epoch: 23
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.75it/s]
[ Tue Jul 19 19:28:19 2022 ]    Mean training loss: 0.2124.
[ Tue Jul 19 19:28:19 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:28:19 2022 ] Eval epoch: 23
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.73it/s]

accuracy:        0.7562883111392941
precision:       0.3709677419354839
recall:          0.06238698010849909
F1:              0.032292853654566175

[ Tue Jul 19 19:30:12 2022 ]    Mean test loss of 760 batches: 0.6321727158893881.
[ Tue Jul 19 19:30:14 2022 ]    Top1: 5.75%
[ Tue Jul 19 19:30:21 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:30:21 2022 ] Training epoch: 24
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.71it/s]
[ Tue Jul 19 19:33:52 2022 ]    Mean training loss: 0.2104.
[ Tue Jul 19 19:33:52 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:33:52 2022 ] Eval epoch: 24
100%|██████████████████████████████████████████████████████| 760/760 [01:53&lt;00:00,  6.72it/s]

accuracy:        0.7679137603043754
precision:       0.5151515151515151
recall:          0.10759493670886076
F1:              0.06831343812788086

[ Tue Jul 19 19:35:45 2022 ]    Mean test loss of 760 batches: 0.5665230429780327.
[ Tue Jul 19 19:35:47 2022 ]    Top1: 8.87%
[ Tue Jul 19 19:35:55 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:35:55 2022 ] Training epoch: 25
100%|██████████████████████████████████████████████████████| 781/781 [03:29&lt;00:00,  3.73it/s]
[ Tue Jul 19 19:39:24 2022 ]    Mean training loss: 0.2081.
[ Tue Jul 19 19:39:24 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:39:24 2022 ] Eval epoch: 25
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.80it/s]

accuracy:        0.7736207989854154
precision:       0.6036585365853658
recall:          0.08951175406871609
F1:              0.0638264618350189

[ Tue Jul 19 19:41:16 2022 ]    Mean test loss of 760 batches: 0.6909860334114024.
[ Tue Jul 19 19:41:18 2022 ]    Top1: 13.62%
[ Tue Jul 19 19:41:26 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:41:26 2022 ] Training epoch: 26
100%|██████████████████████████████████████████████████████| 781/781 [03:26&lt;00:00,  3.78it/s]
[ Tue Jul 19 19:44:53 2022 ]    Mean training loss: 0.2057.
[ Tue Jul 19 19:44:53 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:44:53 2022 ] Eval epoch: 26
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.77it/s]

accuracy:        0.7351511308391461
precision:       0.31592039800995025
recall:          0.11482820976491863
F1:              0.050709920016097386

[ Tue Jul 19 19:46:45 2022 ]    Mean test loss of 760 batches: 0.6817168927016227.
[ Tue Jul 19 19:46:47 2022 ]    Top1: 8.20%
[ Tue Jul 19 19:46:53 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:46:54 2022 ] Training epoch: 27
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.68it/s]
[ Tue Jul 19 19:50:26 2022 ]    Mean training loss: 0.2031.
[ Tue Jul 19 19:50:26 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:50:26 2022 ] Eval epoch: 27
100%|██████████████████████████████████████████████████████| 760/760 [01:54&lt;00:00,  6.64it/s]

accuracy:        0.7645318114563517
precision:       0.45054945054945056
recall:          0.037070524412296565
F1:              0.02245479986374839

[ Tue Jul 19 19:52:20 2022 ]    Mean test loss of 760 batches: 0.7165279691842826.
[ Tue Jul 19 19:52:22 2022 ]    Top1: 9.40%
[ Tue Jul 19 19:52:30 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:52:30 2022 ] Training epoch: 28
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.74it/s]
[ Tue Jul 19 19:55:58 2022 ]    Mean training loss: 0.2011.
[ Tue Jul 19 19:55:58 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:55:58 2022 ] Eval epoch: 28
100%|██████████████████████████████████████████████████████| 760/760 [01:53&lt;00:00,  6.70it/s]

accuracy:        0.7731980553794124
precision:       0.5321285140562249
recall:          0.23960216998191683
F1:              0.14392610324558744

[ Tue Jul 19 19:57:52 2022 ]    Mean test loss of 760 batches: 0.6495097807755595.
[ Tue Jul 19 19:57:54 2022 ]    Top1: 15.44%
[ Tue Jul 19 19:58:04 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:58:04 2022 ] Training epoch: 29
100%|██████████████████████████████████████████████████████| 781/781 [03:29&lt;00:00,  3.72it/s]
[ Tue Jul 19 20:01:34 2022 ]    Mean training loss: 0.1997.
[ Tue Jul 19 20:01:34 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:01:34 2022 ] Eval epoch: 29
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.81it/s]

accuracy:        0.7833439019234834
precision:       0.7352941176470589
recall:          0.11301989150090416
F1:              0.08992288213627994

[ Tue Jul 19 20:03:25 2022 ]    Mean test loss of 760 batches: 0.5679919372655844.
[ Tue Jul 19 20:03:28 2022 ]    Top1: 11.76%
[ Tue Jul 19 20:03:36 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:03:36 2022 ] Training epoch: 30
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 20:07:04 2022 ]    Mean training loss: 0.1980.
[ Tue Jul 19 20:07:04 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:07:04 2022 ] Eval epoch: 30
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.74it/s]

accuracy:        0.7776368632424434
precision:       0.6879432624113475
recall:          0.08770343580470162
F1:              0.06795832505733013

[ Tue Jul 19 20:08:57 2022 ]    Mean test loss of 760 batches: 0.6881330545795591.
[ Tue Jul 19 20:08:59 2022 ]    Top1: 11.45%
[ Tue Jul 19 20:09:06 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:09:06 2022 ] Training epoch: 31
100%|██████████████████████████████████████████████████████| 781/781 [03:29&lt;00:00,  3.73it/s]
[ Tue Jul 19 20:12:36 2022 ]    Mean training loss: 0.1560.
[ Tue Jul 19 20:12:36 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:12:36 2022 ] Eval epoch: 31
100%|██████████████████████████████████████████████████████| 760/760 [01:50&lt;00:00,  6.88it/s]

accuracy:        0.7723525681674065
precision:       0.5353535353535354
recall:          0.19168173598553345
F1:              0.11883659439450027

[ Tue Jul 19 20:14:26 2022 ]    Mean test loss of 760 batches: 0.7085365053364321.
[ Tue Jul 19 20:14:28 2022 ]    Top1: 9.04%
[ Tue Jul 19 20:14:35 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:14:35 2022 ] Training epoch: 32
100%|██████████████████████████████████████████████████████| 781/781 [03:25&lt;00:00,  3.80it/s]
[ Tue Jul 19 20:18:00 2022 ]    Mean training loss: 0.1417.
[ Tue Jul 19 20:18:00 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:18:00 2022 ] Eval epoch: 32
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.83it/s]

accuracy:        0.7689706193193828
precision:       0.5144230769230769
recall:          0.19349005424954793
F1:              0.11655832752187567

[ Tue Jul 19 20:19:52 2022 ]    Mean test loss of 760 batches: 0.7381965774846705.
[ Tue Jul 19 20:19:54 2022 ]    Top1: 10.82%
[ Tue Jul 19 20:20:00 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:20:00 2022 ] Training epoch: 33
100%|██████████████████████████████████████████████████████| 781/781 [03:26&lt;00:00,  3.78it/s]
[ Tue Jul 19 20:23:27 2022 ]    Mean training loss: 0.1347.
[ Tue Jul 19 20:23:27 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:23:27 2022 ] Eval epoch: 33
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.82it/s]

accuracy:        0.7702388501373917
precision:       0.5272727272727272
recall:          0.15732368896925858
F1:              0.09848351777036125

[ Tue Jul 19 20:25:18 2022 ]    Mean test loss of 760 batches: 0.7830234554253126.
[ Tue Jul 19 20:25:20 2022 ]    Top1: 10.59%
[ Tue Jul 19 20:25:27 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:25:27 2022 ] Training epoch: 34
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.74it/s]
[ Tue Jul 19 20:28:56 2022 ]    Mean training loss: 0.1288.
[ Tue Jul 19 20:28:56 2022 ]    Time consumption: [Data]04%, [Network]95%
[ Tue Jul 19 20:28:56 2022 ] Eval epoch: 34
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.76it/s]

accuracy:        0.7712957091523991
precision:       0.5260770975056689
recall:          0.20976491862567812
F1:              0.1271458099568889

[ Tue Jul 19 20:30:49 2022 ]    Mean test loss of 760 batches: 0.7951438913415921.
[ Tue Jul 19 20:30:51 2022 ]    Top1: 11.40%
[ Tue Jul 19 20:30:58 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:30:58 2022 ] Training epoch: 35
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.67it/s]
[ Tue Jul 19 20:34:31 2022 ]    Mean training loss: 0.1244.
[ Tue Jul 19 20:34:31 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:34:31 2022 ] Eval epoch: 35
100%|██████████████████████████████████████████████████████| 760/760 [01:53&lt;00:00,  6.68it/s]

accuracy:        0.7693933629253857
precision:       0.5188172043010753
recall:          0.17450271247739602
F1:              0.1069319607398455

[ Tue Jul 19 20:36:25 2022 ]    Mean test loss of 760 batches: 0.7957099230469842.
[ Tue Jul 19 20:36:26 2022 ]    Top1: 8.38%
[ Tue Jul 19 20:36:33 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:36:33 2022 ] Training epoch: 36
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.75it/s]
[ Tue Jul 19 20:40:01 2022 ]    Mean training loss: 0.1201.
[ Tue Jul 19 20:40:01 2022 ]    Time consumption: [Data]04%, [Network]95%
[ Tue Jul 19 20:40:01 2022 ] Eval epoch: 36
100%|██████████████████████████████████████████████████████| 760/760 [01:53&lt;00:00,  6.72it/s]

accuracy:        0.7846121327414923
precision:       0.583984375
recall:          0.27034358047016277
F1:              0.17027886184900967

[ Tue Jul 19 20:41:55 2022 ]    Mean test loss of 760 batches: 0.7774639136893184.
[ Tue Jul 19 20:41:57 2022 ]    Top1: 15.13%
[ Tue Jul 19 20:42:03 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:42:03 2022 ] Training epoch: 37
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 20:45:31 2022 ]    Mean training loss: 0.1160.
[ Tue Jul 19 20:45:31 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:45:31 2022 ] Eval epoch: 37
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.75it/s]

accuracy:        0.7801733248784612
precision:       0.5876010781671159
recall:          0.19710669077757687
F1:              0.1297916734943576

[ Tue Jul 19 20:47:24 2022 ]    Mean test loss of 760 batches: 0.8954320242530421.
[ Tue Jul 19 20:47:26 2022 ]    Top1: 9.44%
[ Tue Jul 19 20:47:33 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:47:33 2022 ] Training epoch: 38
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 20:51:00 2022 ]    Mean training loss: 0.1128.
[ Tue Jul 19 20:51:00 2022 ]    Time consumption: [Data]04%, [Network]95%
[ Tue Jul 19 20:51:00 2022 ] Eval epoch: 38
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.85it/s]

accuracy:        0.777848235045445
precision:       0.5625
recall:          0.2197106690777577
F1:              0.13868983448538272

[ Tue Jul 19 20:52:52 2022 ]    Mean test loss of 760 batches: 0.8532045789455113.
[ Tue Jul 19 20:52:54 2022 ]    Top1: 10.61%
[ Tue Jul 19 20:53:01 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:53:01 2022 ] Training epoch: 39
100%|██████████████████████████████████████████████████████| 781/781 [03:26&lt;00:00,  3.78it/s]
[ Tue Jul 19 20:56:28 2022 ]    Mean training loss: 0.1090.
[ Tue Jul 19 20:56:28 2022 ]    Time consumption: [Data]04%, [Network]95%
[ Tue Jul 19 20:56:28 2022 ] Eval epoch: 39
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.77it/s]

accuracy:        0.7729866835764109
precision:       0.5316973415132924
recall:          0.23508137432188064
F1:              0.14149156387946424

[ Tue Jul 19 20:58:20 2022 ]    Mean test loss of 760 batches: 0.9168047947907134.
[ Tue Jul 19 20:58:22 2022 ]    Top1: 11.49%
[ Tue Jul 19 20:58:29 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:58:29 2022 ] Training epoch: 40
100%|██████████████████████████████████████████████████████| 781/781 [03:29&lt;00:00,  3.72it/s]
[ Tue Jul 19 21:01:59 2022 ]    Mean training loss: 0.1071.
[ Tue Jul 19 21:01:59 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 21:01:59 2022 ] Eval epoch: 40
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.78it/s]

accuracy:        0.777002747833439
precision:       0.5581395348837209
recall:          0.21699819168173598
F1:              0.13645732155125442

[ Tue Jul 19 21:03:52 2022 ]    Mean test loss of 760 batches: 0.8799759103769534.
[ Tue Jul 19 21:03:53 2022 ]    Top1: 9.37%
[ Tue Jul 19 21:04:00 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:04:01 2022 ] Training epoch: 41
100%|██████████████████████████████████████████████████████| 781/781 [03:31&lt;00:00,  3.69it/s]
[ Tue Jul 19 21:07:32 2022 ]    Mean training loss: 0.1035.
[ Tue Jul 19 21:07:32 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 21:07:32 2022 ] Eval epoch: 41
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.75it/s]

accuracy:        0.7772141196364405
precision:       0.562962962962963
recall:          0.20614828209764918
F1:              0.13120016960317402

[ Tue Jul 19 21:09:25 2022 ]    Mean test loss of 760 batches: 0.9372886403806899.
[ Tue Jul 19 21:09:28 2022 ]    Top1: 8.18%
[ Tue Jul 19 21:09:37 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:09:37 2022 ] Training epoch: 42
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.68it/s]
[ Tue Jul 19 21:13:09 2022 ]    Mean training loss: 0.1012.
[ Tue Jul 19 21:13:09 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 21:13:09 2022 ] Eval epoch: 42
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.73it/s]

accuracy:        0.7664341576833651
precision:       0.5
recall:          0.1763110307414105
F1:              0.1051779935275081

[ Tue Jul 19 21:15:02 2022 ]    Mean test loss of 760 batches: 0.9472922369445625.
[ Tue Jul 19 21:15:04 2022 ]    Top1: 11.52%
[ Tue Jul 19 21:15:11 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:15:11 2022 ] Training epoch: 43
100%|██████████████████████████████████████████████████████| 781/781 [02:57&lt;00:00,  4.41it/s]
[ Tue Jul 19 21:18:08 2022 ]    Mean training loss: 0.0986.
[ Tue Jul 19 21:18:08 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:18:08 2022 ] Eval epoch: 43
100%|██████████████████████████████████████████████████████| 760/760 [01:27&lt;00:00,  8.69it/s]

accuracy:        0.7911646586345381
precision:       0.6163021868787276
recall:          0.28028933092224234
F1:              0.18216144697983527

[ Tue Jul 19 21:19:36 2022 ]    Mean test loss of 760 batches: 0.9471101363434603.
[ Tue Jul 19 21:19:37 2022 ]    Top1: 13.44%
[ Tue Jul 19 21:19:44 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:19:45 2022 ] Training epoch: 44
100%|██████████████████████████████████████████████████████| 781/781 [03:02&lt;00:00,  4.28it/s]
[ Tue Jul 19 21:22:47 2022 ]    Mean training loss: 0.0967.
[ Tue Jul 19 21:22:47 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:22:47 2022 ] Eval epoch: 44
100%|██████████████████████████████████████████████████████| 760/760 [01:29&lt;00:00,  8.52it/s]

accuracy:        0.7839780173324878
precision:       0.5767097966728281
recall:          0.2820976491862568
F1:              0.17504607942744627

[ Tue Jul 19 21:24:16 2022 ]    Mean test loss of 760 batches: 0.911781652507029.
[ Tue Jul 19 21:24:18 2022 ]    Top1: 10.27%
[ Tue Jul 19 21:24:26 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:24:26 2022 ] Training epoch: 45
100%|██████████████████████████████████████████████████████| 781/781 [02:59&lt;00:00,  4.35it/s]
[ Tue Jul 19 21:27:25 2022 ]    Mean training loss: 0.0948.
[ Tue Jul 19 21:27:25 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:27:25 2022 ] Eval epoch: 45
100%|██████████████████████████████████████████████████████| 760/760 [01:29&lt;00:00,  8.51it/s]

accuracy:        0.7630522088353414
precision:       0.4866666666666667
recall:          0.2640144665461121
F1:              0.14678520028370945

[ Tue Jul 19 21:28:55 2022 ]    Mean test loss of 760 batches: 0.9402978484763911.
[ Tue Jul 19 21:28:57 2022 ]    Top1: 11.49%
[ Tue Jul 19 21:29:03 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:29:03 2022 ] Training epoch: 46
100%|██████████████████████████████████████████████████████| 781/781 [02:56&lt;00:00,  4.42it/s]
[ Tue Jul 19 21:32:00 2022 ]    Mean training loss: 0.0930.
[ Tue Jul 19 21:32:00 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:32:00 2022 ] Eval epoch: 46
100%|██████████████████████████████████████████████████████| 760/760 [01:28&lt;00:00,  8.62it/s]

accuracy:        0.7655886704713591
precision:       0.4957446808510638
recall:          0.21066907775768534
F1:              0.12240650802454939

[ Tue Jul 19 21:33:28 2022 ]    Mean test loss of 760 batches: 1.0373078525850647.
[ Tue Jul 19 21:33:30 2022 ]    Top1: 12.05%
[ Tue Jul 19 21:33:36 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:33:36 2022 ] Training epoch: 47
100%|██████████████████████████████████████████████████████| 781/781 [02:56&lt;00:00,  4.43it/s]
[ Tue Jul 19 21:36:33 2022 ]    Mean training loss: 0.0913.
[ Tue Jul 19 21:36:33 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:36:33 2022 ] Eval epoch: 47
100%|██████████████████████████████████████████████████████| 760/760 [01:28&lt;00:00,  8.56it/s]

accuracy:        0.7478334390192348
precision:       0.42
recall:          0.2088607594936709
F1:              0.10770904569474667

[ Tue Jul 19 21:38:02 2022 ]    Mean test loss of 760 batches: 0.9763213669783191.
[ Tue Jul 19 21:38:03 2022 ]    Top1: 10.18%
[ Tue Jul 19 21:38:10 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:38:10 2022 ] Training epoch: 48
100%|██████████████████████████████████████████████████████| 781/781 [02:56&lt;00:00,  4.42it/s]
[ Tue Jul 19 21:41:07 2022 ]    Mean training loss: 0.0896.
[ Tue Jul 19 21:41:07 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:41:07 2022 ] Eval epoch: 48
100%|██████████████████████████████████████████████████████| 760/760 [01:25&lt;00:00,  8.86it/s]

accuracy:        0.7664341576833651
precision:       0.5
recall:          0.21880650994575046
F1:              0.12730142030510258

[ Tue Jul 19 21:42:33 2022 ]    Mean test loss of 760 batches: 0.9845328124338075.
[ Tue Jul 19 21:42:35 2022 ]    Top1: 7.82%
[ Tue Jul 19 21:42:42 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:42:42 2022 ] Training epoch: 49
100%|██████████████████████████████████████████████████████| 781/781 [02:58&lt;00:00,  4.38it/s]
[ Tue Jul 19 21:45:40 2022 ]    Mean training loss: 0.0883.
[ Tue Jul 19 21:45:40 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:45:40 2022 ] Eval epoch: 49
100%|██████████████████████████████████████████████████████| 760/760 [01:27&lt;00:00,  8.64it/s]

accuracy:        0.7706615937433946
precision:       0.5174216027874564
recall:          0.26853526220614826
F1:              0.15559832210858313

[ Tue Jul 19 21:47:08 2022 ]    Mean test loss of 760 batches: 0.9345805480095901.
[ Tue Jul 19 21:47:10 2022 ]    Top1: 10.47%
[ Tue Jul 19 21:47:17 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:47:17 2022 ] Training epoch: 50
100%|██████████████████████████████████████████████████████| 781/781 [02:57&lt;00:00,  4.40it/s]
[ Tue Jul 19 21:50:14 2022 ]    Mean training loss: 0.0872.
[ Tue Jul 19 21:50:14 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:50:14 2022 ] Eval epoch: 50
100%|██████████████████████████████████████████████████████| 760/760 [01:26&lt;00:00,  8.81it/s]

accuracy:        0.7630522088353414
precision:       0.47790055248618785
recall:          0.15641952983725135
F1:              0.09147899550078248

[ Tue Jul 19 21:51:41 2022 ]    Mean test loss of 760 batches: 1.008905115998105.
[ Tue Jul 19 21:51:43 2022 ]    Top1: 9.46%
[ Tue Jul 19 21:51:50 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:51:50 2022 ] Training epoch: 51
100%|██████████████████████████████████████████████████████| 781/781 [02:57&lt;00:00,  4.40it/s]
[ Tue Jul 19 21:54:48 2022 ]    Mean training loss: 0.0696.
[ Tue Jul 19 21:54:48 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:54:48 2022 ] Eval epoch: 51
100%|██████████████████████████████████████████████████████| 760/760 [01:28&lt;00:00,  8.61it/s]

accuracy:        0.7698161065313888
precision:       0.5164609053497943
recall:          0.22694394213381555
F1:              0.1344583548534427

[ Tue Jul 19 21:56:16 2022 ]    Mean test loss of 760 batches: 0.9640071497347794.
[ Tue Jul 19 21:56:18 2022 ]    Top1: 10.58%
[ Tue Jul 19 21:56:25 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:56:25 2022 ] Training epoch: 52
100%|██████████████████████████████████████████████████████| 781/781 [02:58&lt;00:00,  4.37it/s]
[ Tue Jul 19 21:59:23 2022 ]    Mean training loss: 0.0638.
[ Tue Jul 19 21:59:23 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 21:59:23 2022 ] Eval epoch: 52
100%|██████████████████████████████████████████████████████| 760/760 [01:29&lt;00:00,  8.53it/s]

accuracy:        0.7681251321073769
precision:       0.5090497737556561
recall:          0.20343580470162748
F1:              0.12094577806658766

[ Tue Jul 19 22:00:53 2022 ]    Mean test loss of 760 batches: 0.9836037176612177.
[ Tue Jul 19 22:00:55 2022 ]    Top1: 10.97%
[ Tue Jul 19 22:01:01 2022 ]    Top5: 0.00%
[ Tue Jul 19 22:01:02 2022 ] Training epoch: 53
100%|██████████████████████████████████████████████████████| 781/781 [02:56&lt;00:00,  4.43it/s]
[ Tue Jul 19 22:03:58 2022 ]    Mean training loss: 0.0618.
[ Tue Jul 19 22:03:58 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 22:03:58 2022 ] Eval epoch: 53
100%|██████████████████████████████████████████████████████| 760/760 [01:29&lt;00:00,  8.52it/s]

accuracy:        0.7679137603043754
precision:       0.5072765072765073
recall:          0.2206148282097649
F1:              0.12953675640275278

[ Tue Jul 19 22:05:27 2022 ]    Mean test loss of 760 batches: 1.0117301580937286.
[ Tue Jul 19 22:05:29 2022 ]    Top1: 11.30%
[ Tue Jul 19 22:05:36 2022 ]    Top5: 0.00%
[ Tue Jul 19 22:05:36 2022 ] Training epoch: 54
100%|██████████████████████████████████████████████████████| 781/781 [02:55&lt;00:00,  4.46it/s]
[ Tue Jul 19 22:08:31 2022 ]    Mean training loss: 0.0599.
[ Tue Jul 19 22:08:31 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 22:08:31 2022 ] Eval epoch: 54
100%|██████████████████████████████████████████████████████| 760/760 [01:26&lt;00:00,  8.74it/s]

accuracy:        0.7672796448953709
precision:       0.5044052863436124
recall:          0.2070524412296564
F1:              0.12204607128062148

[ Tue Jul 19 22:09:58 2022 ]    Mean test loss of 760 batches: 1.007076105161717.
[ Tue Jul 19 22:10:00 2022 ]    Top1: 11.15%
[ Tue Jul 19 22:10:07 2022 ]    Top5: 0.00%
[ Tue Jul 19 22:10:07 2022 ] Training epoch: 55
100%|██████████████████████████████████████████████████████| 781/781 [02:58&lt;00:00,  4.38it/s]
[ Tue Jul 19 22:13:05 2022 ]    Mean training loss: 0.0589.
[ Tue Jul 19 22:13:05 2022 ]    Time consumption: [Data]05%, [Network]95%
[ Tue Jul 19 22:13:05 2022 ] Eval epoch: 55
100%|██████████████████████████████████████████████████████| 760/760 [01:26&lt;00:00,  8.82it/s]

accuracy:        0.7704502219403931
precision:       0.5207877461706784
recall:          0.21518987341772153
F1:              0.12911255065896546
</code></pre>
<h3 id="骨骼流">骨骼流</h3>
<pre><code>[ Tue Jul 19 17:31:22 2022 ]    Mean test loss of 760 batches: 0.6069661583555372.
[ Tue Jul 19 17:31:24 2022 ]    Top1: 3.66%
[ Tue Jul 19 17:31:33 2022 ]    Top5: 0.00%
[ Tue Jul 19 17:31:33 2022 ] Training epoch: 21
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.66it/s]
[ Tue Jul 19 17:35:06 2022 ]    Mean training loss: 0.2082.
[ Tue Jul 19 17:35:06 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 17:35:06 2022 ] Eval epoch: 21
100%|██████████████████████████████████████████████████████| 760/760 [01:46&lt;00:00,  7.11it/s]

accuracy:        0.7915874022405411
precision:       0.6612466124661247
recall:          0.2206148282097649
F1:              0.15503884043780453

[ Tue Jul 19 17:36:53 2022 ]    Mean test loss of 760 batches: 0.6198260774149706.
[ Tue Jul 19 17:36:55 2022 ]    Top1: 6.01%
[ Tue Jul 19 17:37:02 2022 ]    Top5: 0.00%
[ Tue Jul 19 17:37:02 2022 ] Training epoch: 22
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.66it/s]
[ Tue Jul 19 17:40:36 2022 ]    Mean training loss: 0.2059.
[ Tue Jul 19 17:40:36 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 17:40:36 2022 ] Eval epoch: 22
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.17it/s]

accuracy:        0.8110336081166772
precision:       0.6803418803418804
recall:          0.35985533453887886
F1:              0.2400009696764008

[ Tue Jul 19 17:42:22 2022 ]    Mean test loss of 760 batches: 0.6322289840171211.
[ Tue Jul 19 17:42:24 2022 ]    Top1: 7.14%
[ Tue Jul 19 17:42:31 2022 ]    Top5: 0.00%
[ Tue Jul 19 17:42:31 2022 ] Training epoch: 23
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.70it/s]
[ Tue Jul 19 17:46:02 2022 ]    Mean training loss: 0.2038.
[ Tue Jul 19 17:46:02 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 17:46:02 2022 ] Eval epoch: 23
100%|██████████████████████████████████████████████████████| 760/760 [01:46&lt;00:00,  7.14it/s]

accuracy:        0.7727753117734094
precision:       0.5535714285714286
recall:          0.14014466546112117
F1:              0.09160931154792093

[ Tue Jul 19 17:47:49 2022 ]    Mean test loss of 760 batches: 0.6922783163327135.
[ Tue Jul 19 17:47:51 2022 ]    Top1: 7.24%
[ Tue Jul 19 17:47:58 2022 ]    Top5: 0.00%
[ Tue Jul 19 17:47:58 2022 ] Training epoch: 24
100%|██████████████████████████████████████████████████████| 781/781 [03:34&lt;00:00,  3.65it/s]
[ Tue Jul 19 17:51:32 2022 ]    Mean training loss: 0.2020.
[ Tue Jul 19 17:51:32 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 17:51:32 2022 ] Eval epoch: 24
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.19it/s]

accuracy:        0.7989854153455929
precision:       0.6405109489051095
recall:          0.31735985533453887
F1:              0.20764645107791566

[ Tue Jul 19 17:53:18 2022 ]    Mean test loss of 760 batches: 0.5779038116139801.
[ Tue Jul 19 17:53:20 2022 ]    Top1: 8.93%
[ Tue Jul 19 17:53:26 2022 ]    Top5: 0.00%
[ Tue Jul 19 17:53:26 2022 ] Training epoch: 25
100%|██████████████████████████████████████████████████████| 781/781 [03:34&lt;00:00,  3.65it/s]
[ Tue Jul 19 17:57:00 2022 ]    Mean training loss: 0.2000.
[ Tue Jul 19 17:57:00 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 17:57:00 2022 ] Eval epoch: 25
100%|██████████████████████████████████████████████████████| 760/760 [01:46&lt;00:00,  7.12it/s]

accuracy:        0.797717184527584
precision:       0.6516393442622951
recall:          0.2875226039783002
F1:              0.19323918900842715

[ Tue Jul 19 17:58:47 2022 ]    Mean test loss of 760 batches: 0.6437045866133351.
[ Tue Jul 19 17:58:49 2022 ]    Top1: 14.73%
[ Tue Jul 19 17:58:56 2022 ]    Top5: 0.00%
[ Tue Jul 19 17:58:56 2022 ] Training epoch: 26
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.67it/s]
[ Tue Jul 19 18:02:28 2022 ]    Mean training loss: 0.1979.
[ Tue Jul 19 18:02:28 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:02:28 2022 ] Eval epoch: 26
100%|██████████████████████████████████████████████████████| 760/760 [01:46&lt;00:00,  7.17it/s]

accuracy:        0.7865144789685056
precision:       0.5918762088974855
recall:          0.2766726943942134
F1:              0.17527610347796788

[ Tue Jul 19 18:04:14 2022 ]    Mean test loss of 760 batches: 0.6710291982383321.
[ Tue Jul 19 18:04:16 2022 ]    Top1: 3.03%
[ Tue Jul 19 18:04:23 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:04:23 2022 ] Training epoch: 27
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.65it/s]
[ Tue Jul 19 18:07:57 2022 ]    Mean training loss: 0.1964.
[ Tue Jul 19 18:07:57 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:07:57 2022 ] Eval epoch: 27
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.19it/s]

accuracy:        0.7746776580004228
precision:       0.5506493506493506
recall:          0.19168173598553345
F1:              0.12115885925634383

[ Tue Jul 19 18:09:43 2022 ]    Mean test loss of 760 batches: 0.7039610818910755.
[ Tue Jul 19 18:09:45 2022 ]    Top1: 4.14%
[ Tue Jul 19 18:09:54 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:09:54 2022 ] Training epoch: 28
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.67it/s]
[ Tue Jul 19 18:13:27 2022 ]    Mean training loss: 0.1942.
[ Tue Jul 19 18:13:27 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:13:27 2022 ] Eval epoch: 28
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.22it/s]

accuracy:        0.8116677235256817
precision:       0.6682389937106918
recall:          0.38426763110307416
F1:              0.25021367728565436

[ Tue Jul 19 18:15:12 2022 ]    Mean test loss of 760 batches: 0.6172169858589769.
[ Tue Jul 19 18:15:14 2022 ]    Top1: 14.55%
[ Tue Jul 19 18:15:20 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:15:20 2022 ] Training epoch: 29
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.66it/s]
[ Tue Jul 19 18:18:54 2022 ]    Mean training loss: 0.1930.
[ Tue Jul 19 18:18:54 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:18:54 2022 ] Eval epoch: 29
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.19it/s]

accuracy:        0.7666455294863666
precision:       0.5009784735812133
recall:          0.2314647377938517
F1:              0.13386741945301792

[ Tue Jul 19 18:20:40 2022 ]    Mean test loss of 760 batches: 0.6318130156123325.
[ Tue Jul 19 18:20:42 2022 ]    Top1: 4.60%
[ Tue Jul 19 18:20:49 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:20:49 2022 ] Training epoch: 30
100%|██████████████████████████████████████████████████████| 781/781 [03:35&lt;00:00,  3.63it/s]
[ Tue Jul 19 18:24:24 2022 ]    Mean training loss: 0.1916.
[ Tue Jul 19 18:24:24 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:24:24 2022 ] Eval epoch: 30
100%|██████████████████████████████████████████████████████| 760/760 [01:43&lt;00:00,  7.33it/s]

accuracy:        0.7966603255125766
precision:       0.649895178197065
recall:          0.28028933092224234
F1:              0.18874743197432564

[ Tue Jul 19 18:26:08 2022 ]    Mean test loss of 760 batches: 0.5661559292164288.
[ Tue Jul 19 18:26:10 2022 ]    Top1: 17.75%
[ Tue Jul 19 18:26:19 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:26:19 2022 ] Training epoch: 31
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.66it/s]
[ Tue Jul 19 18:29:53 2022 ]    Mean training loss: 0.1534.
[ Tue Jul 19 18:29:53 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:29:53 2022 ] Eval epoch: 31
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.22it/s]

accuracy:        0.7858803635595012
precision:       0.6045454545454545
recall:          0.24050632911392406
F1:              0.15760750927745035

[ Tue Jul 19 18:31:38 2022 ]    Mean test loss of 760 batches: 0.6746421615446084.
[ Tue Jul 19 18:31:40 2022 ]    Top1: 9.25%
[ Tue Jul 19 18:31:47 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:31:47 2022 ] Training epoch: 32
100%|██████████████████████████████████████████████████████| 781/781 [03:31&lt;00:00,  3.70it/s]
[ Tue Jul 19 18:35:18 2022 ]    Mean training loss: 0.1394.
[ Tue Jul 19 18:35:18 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:35:18 2022 ] Eval epoch: 32
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.20it/s]

accuracy:        0.7814415556964701
precision:       0.5738045738045738
recall:          0.24954792043399637
F1:              0.15706424136956984

[ Tue Jul 19 18:37:04 2022 ]    Mean test loss of 760 batches: 0.7187577111548499.
[ Tue Jul 19 18:37:05 2022 ]    Top1: 7.70%
[ Tue Jul 19 18:37:12 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:37:12 2022 ] Training epoch: 33
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.70it/s]
[ Tue Jul 19 18:40:43 2022 ]    Mean training loss: 0.1326.
[ Tue Jul 19 18:40:43 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:40:43 2022 ] Eval epoch: 33
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.27it/s]

accuracy:        0.777002747833439
precision:       0.5553097345132744
recall:          0.22694394213381555
F1:              0.14142114773785874

[ Tue Jul 19 18:42:27 2022 ]    Mean test loss of 760 batches: 0.7197429396604237.
[ Tue Jul 19 18:42:29 2022 ]    Top1: 9.52%
[ Tue Jul 19 18:42:36 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:42:36 2022 ] Training epoch: 34
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.71it/s]
[ Tue Jul 19 18:46:06 2022 ]    Mean training loss: 0.1276.
[ Tue Jul 19 18:46:06 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:46:06 2022 ] Eval epoch: 34
100%|██████████████████████████████████████████████████████| 760/760 [01:46&lt;00:00,  7.14it/s]

accuracy:        0.7776368632424434
precision:       0.5609195402298851
recall:          0.2206148282097649
F1:              0.13892201037434926

[ Tue Jul 19 18:47:53 2022 ]    Mean test loss of 760 batches: 0.7541275090881084.
[ Tue Jul 19 18:47:55 2022 ]    Top1: 7.65%
[ Tue Jul 19 18:48:01 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:48:01 2022 ] Training epoch: 35
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.70it/s]
[ Tue Jul 19 18:51:32 2022 ]    Mean training loss: 0.1237.
[ Tue Jul 19 18:51:32 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:51:32 2022 ] Eval epoch: 35
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.20it/s]

accuracy:        0.781864299302473
precision:       0.5816554809843401
recall:          0.23508137432188064
F1:              0.1505296371923996

[ Tue Jul 19 18:53:18 2022 ]    Mean test loss of 760 batches: 0.7902741006134372.
[ Tue Jul 19 18:53:20 2022 ]    Top1: 8.93%
[ Tue Jul 19 18:53:26 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:53:26 2022 ] Training epoch: 36
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.68it/s]
[ Tue Jul 19 18:56:58 2022 ]    Mean training loss: 0.1200.
[ Tue Jul 19 18:56:58 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 18:56:58 2022 ] Eval epoch: 36
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.28it/s]

accuracy:        0.7812301838934687
precision:       0.5767543859649122
recall:          0.23779385171790235
F1:              0.1511656114570198

[ Tue Jul 19 18:58:43 2022 ]    Mean test loss of 760 batches: 0.7907348420961122.
[ Tue Jul 19 18:58:45 2022 ]    Top1: 10.95%
[ Tue Jul 19 18:58:51 2022 ]    Top5: 0.00%
[ Tue Jul 19 18:58:51 2022 ] Training epoch: 37
100%|██████████████████████████████████████████████████████| 781/781 [03:31&lt;00:00,  3.69it/s]
[ Tue Jul 19 19:02:23 2022 ]    Mean training loss: 0.1163.
[ Tue Jul 19 19:02:23 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:02:23 2022 ] Eval epoch: 37
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.20it/s]

accuracy:        0.7708729655463961
precision:       0.5254237288135594
recall:          0.1962025316455696
F1:              0.11975824038880928

[ Tue Jul 19 19:04:09 2022 ]    Mean test loss of 760 batches: 0.8390756560979705.
[ Tue Jul 19 19:04:11 2022 ]    Top1: 10.62%
[ Tue Jul 19 19:04:17 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:04:17 2022 ] Training epoch: 38
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.74it/s]
[ Tue Jul 19 19:07:46 2022 ]    Mean training loss: 0.1134.
[ Tue Jul 19 19:07:46 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:07:46 2022 ] Eval epoch: 38
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.31it/s]

accuracy:        0.7824984147114775
precision:       0.6032608695652174
recall:          0.2007233273056058
F1:              0.13424566488159123

[ Tue Jul 19 19:09:31 2022 ]    Mean test loss of 760 batches: 0.8317653734433024.
[ Tue Jul 19 19:09:32 2022 ]    Top1: 10.79%
[ Tue Jul 19 19:09:38 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:09:39 2022 ] Training epoch: 39
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.71it/s]
[ Tue Jul 19 19:13:09 2022 ]    Mean training loss: 0.1104.
[ Tue Jul 19 19:13:09 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:13:09 2022 ] Eval epoch: 39
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.19it/s]

accuracy:        0.7736207989854154
precision:       0.5341365461847389
recall:          0.24050632911392406
F1:              0.1447764186885903

[ Tue Jul 19 19:14:55 2022 ]    Mean test loss of 760 batches: 0.8426117130604229.
[ Tue Jul 19 19:14:57 2022 ]    Top1: 9.64%
[ Tue Jul 19 19:15:03 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:15:03 2022 ] Training epoch: 40
100%|██████████████████████████████████████████████████████| 781/781 [03:32&lt;00:00,  3.67it/s]
[ Tue Jul 19 19:18:36 2022 ]    Mean training loss: 0.1085.
[ Tue Jul 19 19:18:36 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:18:36 2022 ] Eval epoch: 40
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.26it/s]

accuracy:        0.7767913760304376
precision:       0.5511482254697286
recall:          0.23869801084990958
F1:              0.1470047900579405

[ Tue Jul 19 19:20:21 2022 ]    Mean test loss of 760 batches: 0.8183515262348872.
[ Tue Jul 19 19:20:23 2022 ]    Top1: 8.79%
[ Tue Jul 19 19:20:30 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:20:30 2022 ] Training epoch: 41
100%|██████████████████████████████████████████████████████| 781/781 [03:31&lt;00:00,  3.69it/s]
[ Tue Jul 19 19:24:02 2022 ]    Mean training loss: 0.1055.
[ Tue Jul 19 19:24:02 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:24:02 2022 ] Eval epoch: 41
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.29it/s]

accuracy:        0.7844007609384908
precision:       0.5848303393213573
recall:          0.26491862567811936
F1:              0.16751727145626596

[ Tue Jul 19 19:25:46 2022 ]    Mean test loss of 760 batches: 0.8089341167556612.
[ Tue Jul 19 19:25:48 2022 ]    Top1: 9.74%
[ Tue Jul 19 19:25:55 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:25:55 2022 ] Training epoch: 42
100%|██████████████████████████████████████████████████████| 781/781 [03:31&lt;00:00,  3.69it/s]
[ Tue Jul 19 19:29:26 2022 ]    Mean training loss: 0.1035.
[ Tue Jul 19 19:29:26 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:29:26 2022 ] Eval epoch: 42
100%|██████████████████████████████████████████████████████| 760/760 [01:48&lt;00:00,  7.01it/s]

accuracy:        0.7776368632424434
precision:       0.5582417582417583
recall:          0.22965641952983726
F1:              0.14341286883533175

[ Tue Jul 19 19:31:15 2022 ]    Mean test loss of 760 batches: 0.833223547237484.
[ Tue Jul 19 19:31:17 2022 ]    Top1: 11.76%
[ Tue Jul 19 19:31:24 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:31:24 2022 ] Training epoch: 43
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.66it/s]
[ Tue Jul 19 19:34:58 2022 ]    Mean training loss: 0.1014.
[ Tue Jul 19 19:34:58 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:34:58 2022 ] Eval epoch: 43
100%|██████████████████████████████████████████████████████| 760/760 [01:48&lt;00:00,  6.98it/s]

accuracy:        0.7723525681674065
precision:       0.526615969581749
recall:          0.2504520795660036
F1:              0.14843783250469136

[ Tue Jul 19 19:36:47 2022 ]    Mean test loss of 760 batches: 0.8829739411998736.
[ Tue Jul 19 19:36:49 2022 ]    Top1: 9.51%
[ Tue Jul 19 19:36:57 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:36:57 2022 ] Training epoch: 44
100%|██████████████████████████████████████████████████████| 781/781 [03:35&lt;00:00,  3.63it/s]
[ Tue Jul 19 19:40:32 2022 ]    Mean training loss: 0.0997.
[ Tue Jul 19 19:40:32 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:40:32 2022 ] Eval epoch: 44
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.23it/s]

accuracy:        0.7753117734094271
precision:       0.5405405405405406
recall:          0.25316455696202533
F1:              0.15258439824527942

[ Tue Jul 19 19:42:18 2022 ]    Mean test loss of 760 batches: 0.8593113544053937.
[ Tue Jul 19 19:42:20 2022 ]    Top1: 8.17%
[ Tue Jul 19 19:42:27 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:42:27 2022 ] Training epoch: 45
100%|██████████████████████████████████████████████████████| 781/781 [03:29&lt;00:00,  3.72it/s]
[ Tue Jul 19 19:45:57 2022 ]    Mean training loss: 0.0982.
[ Tue Jul 19 19:45:57 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:45:57 2022 ] Eval epoch: 45
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.22it/s]

accuracy:        0.7689706193193828
precision:       0.5111524163568774
recall:          0.24864376130198915
F1:              0.14444270423482136

[ Tue Jul 19 19:47:42 2022 ]    Mean test loss of 760 batches: 0.9002747813141659.
[ Tue Jul 19 19:47:44 2022 ]    Top1: 9.70%
[ Tue Jul 19 19:47:51 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:47:51 2022 ] Training epoch: 46
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.65it/s]
[ Tue Jul 19 19:51:25 2022 ]    Mean training loss: 0.0966.
[ Tue Jul 19 19:51:25 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:51:25 2022 ] Eval epoch: 46
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.20it/s]

accuracy:        0.7712957091523991
precision:       0.5236139630390144
recall:          0.23056057866184448
F1:              0.13764278917950928

[ Tue Jul 19 19:53:11 2022 ]    Mean test loss of 760 batches: 0.8921237524990973.
[ Tue Jul 19 19:53:13 2022 ]    Top1: 9.67%
[ Tue Jul 19 19:53:19 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:53:19 2022 ] Training epoch: 47
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.71it/s]
[ Tue Jul 19 19:56:50 2022 ]    Mean training loss: 0.0951.
[ Tue Jul 19 19:56:50 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 19:56:50 2022 ] Eval epoch: 47
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.23it/s]

accuracy:        0.7751004016064257
precision:       0.5442764578833693
recall:          0.22784810126582278
F1:              0.1399589626498403

[ Tue Jul 19 19:58:36 2022 ]    Mean test loss of 760 batches: 0.8940767494667518.
[ Tue Jul 19 19:58:38 2022 ]    Top1: 8.69%
[ Tue Jul 19 19:58:45 2022 ]    Top5: 0.00%
[ Tue Jul 19 19:58:45 2022 ] Training epoch: 48
100%|██████████████████████████████████████████████████████| 781/781 [03:29&lt;00:00,  3.73it/s]
[ Tue Jul 19 20:02:14 2022 ]    Mean training loss: 0.0936.
[ Tue Jul 19 20:02:14 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:02:14 2022 ] Eval epoch: 48
100%|██████████████████████████████████████████████████████| 760/760 [01:43&lt;00:00,  7.36it/s]

accuracy:        0.777848235045445
precision:       0.5569620253164557
recall:          0.23869801084990958
F1:              0.14807449615664078

[ Tue Jul 19 20:03:57 2022 ]    Mean test loss of 760 batches: 0.8903453569859267.
[ Tue Jul 19 20:03:59 2022 ]    Top1: 14.59%
[ Tue Jul 19 20:04:07 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:04:07 2022 ] Training epoch: 49
100%|██████████████████████████████████████████████████████| 781/781 [03:30&lt;00:00,  3.71it/s]
[ Tue Jul 19 20:07:38 2022 ]    Mean training loss: 0.0927.
[ Tue Jul 19 20:07:38 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:07:38 2022 ] Eval epoch: 49
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.25it/s]

accuracy:        0.7660114140773621
precision:       0.4979919678714859
recall:          0.22423146473779385
F1:              0.1296759366632229

[ Tue Jul 19 20:09:23 2022 ]    Mean test loss of 760 batches: 0.8835280731320381.
[ Tue Jul 19 20:09:25 2022 ]    Top1: 7.44%
[ Tue Jul 19 20:09:32 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:09:32 2022 ] Training epoch: 50
100%|██████████████████████████████████████████████████████| 781/781 [03:28&lt;00:00,  3.74it/s]
[ Tue Jul 19 20:13:01 2022 ]    Mean training loss: 0.0912.
[ Tue Jul 19 20:13:01 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:13:01 2022 ] Eval epoch: 50
100%|██████████████████████████████████████████████████████| 760/760 [01:44&lt;00:00,  7.25it/s]

accuracy:        0.777425491439442
precision:       0.565989847715736
recall:          0.20162748643761302
F1:              0.12912196337878962

[ Tue Jul 19 20:14:46 2022 ]    Mean test loss of 760 batches: 0.9798662955039426.
[ Tue Jul 19 20:14:48 2022 ]    Top1: 7.73%
[ Tue Jul 19 20:14:54 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:14:55 2022 ] Training epoch: 51
100%|██████████████████████████████████████████████████████| 781/781 [03:26&lt;00:00,  3.77it/s]
[ Tue Jul 19 20:18:21 2022 ]    Mean training loss: 0.0764.
[ Tue Jul 19 20:18:21 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:18:21 2022 ] Eval epoch: 51
100%|██████████████████████████████████████████████████████| 760/760 [01:45&lt;00:00,  7.22it/s]

accuracy:        0.7757345170154302
precision:       0.5497737556561086
recall:          0.2197106690777577
F1:              0.13652695441471605

[ Tue Jul 19 20:20:07 2022 ]    Mean test loss of 760 batches: 0.9206365417296949.
[ Tue Jul 19 20:20:09 2022 ]    Top1: 9.72%
[ Tue Jul 19 20:20:16 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:20:16 2022 ] Training epoch: 52
100%|███████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.77it/s]
[ Tue Jul 19 20:23:43 2022 ]    Mean training loss: 0.0712.
[ Tue Jul 19 20:23:43 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:23:43 2022 ] Eval epoch: 52
100%|███████████████████████████████████████| 760/760 [01:46&lt;00:00,  7.11it/s]

accuracy:        0.7801733248784612
precision:       0.5704989154013015
recall:          0.23779385171790235
F1:              0.15004333032895256

[ Tue Jul 19 20:25:30 2022 ]    Mean test loss of 760 batches: 0.9085176489462978.
[ Tue Jul 19 20:25:32 2022 ]    Top1: 9.83%
[ Tue Jul 19 20:25:40 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:25:40 2022 ] Training epoch: 53
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 20:29:08 2022 ]    Mean training loss: 0.0693.
[ Tue Jul 19 20:29:08 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:29:08 2022 ] Eval epoch: 53
100%|██████████████████████████████████████████████████████| 760/760 [01:47&lt;00:00,  7.05it/s]

accuracy:        0.7780596068484464
precision:       0.5591397849462365
recall:          0.23508137432188064
F1:              0.1465185586561907

[ Tue Jul 19 20:30:56 2022 ]    Mean test loss of 760 batches: 0.9265568680080928.
[ Tue Jul 19 20:30:58 2022 ]    Top1: 9.81%
[ Tue Jul 19 20:31:05 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:31:05 2022 ] Training epoch: 54
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 20:34:33 2022 ]    Mean training loss: 0.0677.
[ Tue Jul 19 20:34:33 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:34:33 2022 ] Eval epoch: 54
100%|██████████████████████████████████████████████████████| 760/760 [01:49&lt;00:00,  6.96it/s]

accuracy:        0.7729866835764109
precision:       0.5342163355408388
recall:          0.21880650994575046
F1:              0.13335823002723454

[ Tue Jul 19 20:36:22 2022 ]    Mean test loss of 760 batches: 0.9319131665716046.
[ Tue Jul 19 20:36:25 2022 ]    Top1: 9.95%
[ Tue Jul 19 20:36:32 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:36:32 2022 ] Training epoch: 55
100%|██████████████████████████████████████████████████████| 781/781 [03:26&lt;00:00,  3.78it/s]
[ Tue Jul 19 20:39:58 2022 ]    Mean training loss: 0.0666.
[ Tue Jul 19 20:39:58 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:39:58 2022 ] Eval epoch: 55
100%|██████████████████████████████████████████████████████| 760/760 [01:49&lt;00:00,  6.92it/s]

accuracy:        0.777848235045445
precision:       0.5597345132743363
recall:          0.22875226039783003
F1:              0.14318309424373046

[ Tue Jul 19 20:41:49 2022 ]    Mean test loss of 760 batches: 0.9159731537495789.
[ Tue Jul 19 20:41:51 2022 ]    Top1: 10.06%
[ Tue Jul 19 20:41:59 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:41:59 2022 ] Training epoch: 56
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.77it/s]
[ Tue Jul 19 20:45:26 2022 ]    Mean training loss: 0.0655.
[ Tue Jul 19 20:45:26 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:45:26 2022 ] Eval epoch: 56
100%|██████████████████████████████████████████████████████| 760/760 [01:50&lt;00:00,  6.89it/s]

accuracy:        0.7772141196364405
precision:       0.5557986870897156
recall:          0.22965641952983726
F1:              0.14298061707984097

[ Tue Jul 19 20:47:17 2022 ]    Mean test loss of 760 batches: 0.9402584807457108.
[ Tue Jul 19 20:47:19 2022 ]    Top1: 9.71%
[ Tue Jul 19 20:47:28 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:47:28 2022 ] Training epoch: 57
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.77it/s]
[ Tue Jul 19 20:50:55 2022 ]    Mean training loss: 0.0651.
[ Tue Jul 19 20:50:55 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:50:55 2022 ] Eval epoch: 57
100%|██████████████████████████████████████████████████████| 760/760 [01:47&lt;00:00,  7.04it/s]

accuracy:        0.7731980553794124
precision:       0.5343347639484979
recall:          0.22513562386980107
F1:              0.13674318280063166

[ Tue Jul 19 20:52:43 2022 ]    Mean test loss of 760 batches: 0.9236244497722701.
[ Tue Jul 19 20:52:45 2022 ]    Top1: 10.37%
[ Tue Jul 19 20:52:54 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:52:54 2022 ] Training epoch: 58
100%|██████████████████████████████████████████████████████| 781/781 [03:24&lt;00:00,  3.81it/s]
[ Tue Jul 19 20:56:19 2022 ]    Mean training loss: 0.0637.
[ Tue Jul 19 20:56:19 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 20:56:19 2022 ] Eval epoch: 58
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.84it/s]

accuracy:        0.7683365039103783
precision:       0.5095541401273885
recall:          0.21699819168173598
F1:              0.1280845357936238

[ Tue Jul 19 20:58:10 2022 ]    Mean test loss of 760 batches: 0.9415992480555647.
[ Tue Jul 19 20:58:13 2022 ]    Top1: 10.15%
[ Tue Jul 19 20:58:21 2022 ]    Top5: 0.00%
[ Tue Jul 19 20:58:21 2022 ] Training epoch: 59
100%|██████████████████████████████████████████████████████| 781/781 [03:27&lt;00:00,  3.76it/s]
[ Tue Jul 19 21:01:49 2022 ]    Mean training loss: 0.0632.
[ Tue Jul 19 21:01:49 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 21:01:49 2022 ] Eval epoch: 59
100%|██████████████████████████████████████████████████████| 760/760 [01:51&lt;00:00,  6.82it/s]

accuracy:        0.7776368632424434
precision:       0.556989247311828
recall:          0.23417721518987342
F1:              0.14564161797001451

[ Tue Jul 19 21:03:41 2022 ]    Mean test loss of 760 batches: 0.9461381932230373.
[ Tue Jul 19 21:03:44 2022 ]    Top1: 9.88%
[ Tue Jul 19 21:03:54 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:03:54 2022 ] Training epoch: 60
100%|██████████████████████████████████████████████████████| 781/781 [03:31&lt;00:00,  3.70it/s]
[ Tue Jul 19 21:07:25 2022 ]    Mean training loss: 0.0628.
[ Tue Jul 19 21:07:25 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 21:07:25 2022 ] Eval epoch: 60
100%|██████████████████████████████████████████████████████| 760/760 [01:52&lt;00:00,  6.73it/s]

accuracy:        0.7738321707884168
precision:       0.5386313465783664
recall:          0.2206148282097649
F1:              0.13509202259097314

[ Tue Jul 19 21:09:18 2022 ]    Mean test loss of 760 batches: 0.9629002280533314.
[ Tue Jul 19 21:09:21 2022 ]    Top1: 9.72%
[ Tue Jul 19 21:09:30 2022 ]    Top5: 0.00%
[ Tue Jul 19 21:09:30 2022 ] Training epoch: 61
100%|██████████████████████████████████████████████████████| 781/781 [03:33&lt;00:00,  3.66it/s]
[ Tue Jul 19 21:13:03 2022 ]    Mean training loss: 0.0607.
[ Tue Jul 19 21:13:03 2022 ]    Time consumption: [Data]04%, [Network]96%
[ Tue Jul 19 21:13:03 2022 ] Eval epoch: 61
100%|██████████████████████████████████████████████████████| 760/760 [01:50&lt;00:00,  6.90it/s]

accuracy:        0.7784823504544494
precision:       0.5629139072847682
recall:          0.23056057866184448
F1:              0.14473108730272666

[ Tue Jul 19 21:14:53 2022 ]    Mean test loss of 760 batches: 0.9635843319328208.
[ Tue Jul 19 21:14:56 2022 ]    Top1: 10.69%
[ Tue Jul 19 21:15:05 2022 ]    Top5: 0.00%
best accuracy:  0.8118790953286832  model_name:  ./runs/PF_agcn_bone
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[OMP数据集预处理]]></title>
        <id>https://akasaka47.github.io/post/omp-shu-ju-ji-yu-chu-li/</id>
        <link href="https://akasaka47.github.io/post/omp-shu-ju-ji-yu-chu-li/">
        </link>
        <updated>2022-07-19T07:55:16.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#omp%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86">OMP数据集预处理</a>
<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86-a">数据集-A</a>
<ul>
<li><a href="#%E6%89%8B%E9%AA%A8%E6%9E%B6%E6%95%B0%E6%8D%AE">手骨架数据</a></li>
<li><a href="#10%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE">10分类标签</a></li>
<li><a href="#88%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE">88分类标签</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86-b">数据集-B</a>
<ul>
<li><a href="#%E6%89%8B%E9%AA%A8%E6%9E%B6%E6%95%B0%E6%8D%AE-2">手骨架数据</a></li>
<li><a href="#88%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE-2">88分类标签</a></li>
<li><a href="#10%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE-2">10分类标签</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h1 id="omp数据集预处理">OMP数据集预处理</h1>
<blockquote>
<p>Open MIDI Piano 钢琴视频转录数据集</p>
</blockquote>
<hr>
<h2 id="数据集-a">数据集-A</h2>
<p>视频分辨率：1280 x 720</p>
<p>数据总数：265508</p>
<p>视频分帧，保存数据及标签</p>
<hr>
<h3 id="手骨架数据">手骨架数据</h3>
<ul>
<li>维度：(265508, 3, 5, 42, 1)</li>
<li>265508：数据条数</li>
<li>3：关节点X坐标，Y坐标，置信度
<ul>
<li>原点在图像的左上角</li>
<li>X，Y坐标关于图片尺寸归一化，且-0.5</li>
</ul>
</li>
<li>5：前2帧 + 当前帧 + 后2帧</li>
<li>42: 左右手各21个关节点
<ul>
<li>左手对应范围：0~20</li>
<li>右手对应范围：21~41</li>
<li>左手指尖：[4, 8, 12, 16, 20]</li>
<li>右手指尖：[25, 29, 33, 37, 41]</li>
</ul>
</li>
<li>1：1双手</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2022/07/14/K1IBcUw24DMH5YX.png" alt="手骨架.png" loading="lazy"></figure>
<hr>
<h3 id="10分类标签">10分类标签</h3>
<ul>
<li>维度：(265508, 2)</li>
<li>格式<pre><code class="language-py">[
  [ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'img_path_0' ],
  [ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'img_path_1' ],
  [ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'img_path_2' ],
  ...
]
</code></pre>
</li>
<li>img_path：该标签对应视频帧图片的路径</li>
<li>10长度向量
<ul>
<li>前5个为左手，后5个为右手</li>
<li>对应关系如图所示</li>
</ul>
</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://s2.loli.net/2022/07/19/fTjFZLdaM72HAnU.png" alt="手指10标签下标.png" loading="lazy"></figure>
<hr>
<h3 id="88分类标签">88分类标签</h3>
<ul>
<li>维度：(265508, 2)</li>
<li>格式<pre><code class="language-py">[
  [ [88 个 0/1], 'img_path_0' ],
  [ [88 个 0/1], 'img_path_1' ],
  [ [88 个 0/1], 'img_path_2' ],
  ...
]
</code></pre>
</li>
<li>img_path：该标签对应视频帧图片的路径</li>
<li>88长度向量：对应88个钢琴键，从左到右</li>
</ul>
<hr>
<h2 id="数据集-b">数据集-B</h2>
<p>视频分辨率：1920 x 1080</p>
<p>数据总数：389081</p>
<hr>
<h3 id="手骨架数据-2">手骨架数据</h3>
<p>源数据存放路径：<code>poseJson/{视频名}/{序号}.json</code><br>
例：<code>poseJson/yty1_1/00000.json</code><br>
json 格式：</p>
<pre><code class="language-json">{
  &quot;skeleton&quot;: [
    {
      &quot;pose&quot;: [x坐标, y坐标, x坐标, y坐标...],  // 21组坐标，42个数据
      &quot;score&quot;: [置信度...]    // 21个数据
    },
    {
      &quot;pose&quot;: [x坐标, y坐标, x坐标, y坐标...],
      &quot;score&quot;: [置信度...]
    }
  ],
  &quot;flag&quot;: [&quot;left&quot;, &quot;right&quot;]  // 左右手
}
</code></pre>
<p>坐标数据已经关于图片尺寸归一化，需要<code>-0.5</code>与数据集-A匹配</p>
<p>每一帧的骨架数据，需要该帧的前2帧和后2帧，共计5帧的数据<br>
对每一个视频，丢弃前2帧和最后2帧</p>
<p>生成适用于2s-AGCN数据格式：<code>(389081, 3, 5, 42, 1)</code></p>
<p>校对：</p>
<p>数据集-A，手骨架</p>
<figure data-type="image" tabindex="3"><img src="https://s2.loli.net/2022/07/19/4TSyDZnBckmq6f2.png" alt="数据集-A-手骨架.png" loading="lazy"></figure>
<p>数据集-B，手骨架</p>
<figure data-type="image" tabindex="4"><img src="https://s2.loli.net/2022/07/19/XzWtoEV2mCyI4lq.png" alt="数据集-B-手骨架.png" loading="lazy"></figure>
<p>数据集-B格式与A相同</p>
<hr>
<h3 id="88分类标签-2">88分类标签</h3>
<p>源数据存放路径：<code>label/{视频名}.txt</code><br>
例：<code>label/yty1_1.txt</code><br>
txt格式：</p>
<pre><code>img_path_1 num_1 num_2...
img_path_2 num_1 num_2...
img_path_3 num_1 num_2...

例：playing3/00091.jpg 52 49

img_path: 视频帧图片路径
num: 钢琴键音高，范围1~88
</code></pre>
<p>转换为与数据集-A相同的格式：</p>
<ul>
<li>维度：(265508, 2)</li>
<li>格式<pre><code class="language-py">[
  [ [88 个 0/1], 'img_path_0' ],
  [ [88 个 0/1], 'img_path_1' ],
  [ [88 个 0/1], 'img_path_2' ],
  ...
]
</code></pre>
</li>
<li>img_path：该标签对应视频帧图片的路径</li>
<li>88长度向量：对应88个钢琴键，从左到右</li>
</ul>
<hr>
<h3 id="10分类标签-2">10分类标签</h3>
<p>钢琴88键在图片的横向上均匀分布<br>
将横向尺寸平均分割，得到88个琴键的中心位置<br>
当有琴键按下时，根据x坐标判断，取离该琴键最近的指尖，判断该手指有按下</p>
<p>校对：</p>
<p>数据集-A</p>
<figure data-type="image" tabindex="5"><img src="https://s2.loli.net/2022/07/19/vcLGPKMAOhWC7mj.png" alt="数据集A-10标签验证.png" loading="lazy"></figure>
<p>数据集-B</p>
<figure data-type="image" tabindex="6"><img src="https://s2.loli.net/2022/07/19/9zZ6CRfxABGTL57.png" alt="数据集B-10标签验证.png" loading="lazy"></figure>
<p>格式与数据集-A相同，且与图片相符</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[图卷积学习之路-2（2s-AGCN）]]></title>
        <id>https://akasaka47.github.io/post/gcn-xue-xi-zhi-lu-2-2s-agcn/</id>
        <link href="https://akasaka47.github.io/post/gcn-xue-xi-zhi-lu-2-2s-agcn/">
        </link>
        <updated>2022-07-13T11:20:46.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#2s-agcn-2019">2s-AGCN 2019</a>
<ul>
<li><a href="#%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5">邻接矩阵</a></li>
<li><a href="#%E5%8F%8C%E6%B5%81%E7%9A%84-2-%E5%88%86%E6%94%AF">双流的 2 分支</a></li>
<li><a href="#%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE">输入数据</a></li>
<li><a href="#%E6%BA%90%E7%A0%81%E4%BF%AE%E6%94%B9">源码修改</a>
<ul>
<li><a href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">配置文件</a></li>
<li><a href="#%E9%AA%A8%E9%AA%BC%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90">骨骼数据生成</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE">网络配置</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89">网络模型定义</a></li>
<li><a href="#%E4%B8%BB%E6%96%87%E4%BB%B6">主文件</a></li>
</ul>
</li>
<li><a href="#%E4%BD%BF%E7%94%A8%E9%A1%BA%E5%BA%8F">使用顺序</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="2s-agcn-2019">2s-AGCN 2019</h1>
<blockquote>
<p>Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition<br>
CVPR 2019</p>
</blockquote>
<p><strong>双流图卷积动作识别</strong></p>
<p><a href="https://arxiv.org/abs/1805.07694">论文地址</a><br>
<a href="https://github.com/lshiwjx/2s-AGCN">源码地址</a><br>
<a href="https://zhuanlan.zhihu.com/p/179956749">论文解读</a></p>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2022/07/13/DU9b6QHLjaA1kmv.png" alt="2s-AGCN_1.png" loading="lazy"></figure>
<hr>
<h2 id="邻接矩阵">邻接矩阵</h2>
<figure data-type="image" tabindex="2"><img src="https://s2.loli.net/2022/07/13/3ETcSAxCJwfrqD9.png" alt="2s-AGCN-邻接矩阵.png" loading="lazy"></figure>
<ul>
<li>A：与 ST-GCN 相同，规定了物理存在的连接</li>
<li>B：一个 N*N 的矩阵，参数无约束条件，完全由网络学习得到，可以创造不存在的连接</li>
<li>C：对每一个样本学习一个独有的图，有归一化处理，范围在 0~1</li>
</ul>
<hr>
<h2 id="双流的-2-分支">双流的 2 分支</h2>
<ul>
<li>流 1
<ul>
<li>与 ST-GCN 相同，骨架的节点数据</li>
</ul>
</li>
<li>流 2
<ul>
<li>输入数据：骨骼的长度和方向</li>
<li>手的 21 点，以手腕处的点为中心点，每个骨骼靠近中心点的关节为出发点，另一个为终点</li>
<li>可得到骨骼向量，以向量的长度和方向作为输入数据</li>
<li>关节数比骨骼数多 1，添加一个空骨骼，使骨骼数据维度与关节维度相同</li>
</ul>
</li>
</ul>
<hr>
<h2 id="输入数据">输入数据</h2>
<ul>
<li><code>N：Batch Size</code></li>
<li><code>C：channels</code> 通道数，这里 3 通道，对应 X 坐标，Y 坐标，置信度</li>
<li><code>T：Frames' Number</code> 骨架图数</li>
<li><code>V：Joints‘ Number</code> 一张骨架图内，节点数</li>
<li><code>M：People's Number</code> 一张骨架图内，人数</li>
</ul>
<hr>
<h2 id="源码修改">源码修改</h2>
<p>以 <code>NTU-RGB-D</code> 网络为基础进行修改</p>
<h3 id="配置文件">配置文件</h3>
<p><code>config/nturgbd-cross-view/train_joint.yaml</code></p>
<pre><code class="language-py"># 训练关节

work_dir: './work_dir/{...}/agcn_bone'     # 括号内填自定义数据集名称，如 piano_finger
model_saved_name: './runs/{...}_agcn_bone'

train_feeder_args:
  data_path: './data/{...}/train_data_bone.npy'
  label_path: './data/{...}/train_label.pkl'

test_feeder_args:
  data_path: './data/{...}/val_data_bone.npy'
  label_path: './data/{...}/val_label.pkl'

model_args:
  num_class: 10   # 10根手指
  num_point: 42   # 一只手21个关节点，2只手共计42个关节点
  num_person: 1

device: [0, 1, 2, 3]    # 多GPU环境下，选择使用哪些GPU
batch_size: 256         # 根据硬件配置，选择合适的batch_size
test_batch_size: 256
num_epoch: 50           # 迭代次数
</code></pre>
<p><code>config/nturgbd-cross-view/train_bone.yaml</code><br>
<code>config/nturgbd-cross-view/test_joint.yaml</code><br>
<code>config/nturgbd-cross-view/test_bone.yaml</code></p>
<p>修改基本同上</p>
<hr>
<h3 id="骨骼数据生成">骨骼数据生成</h3>
<p><code>data_gen/gen_bone_data.py</code></p>
<pre><code class="language-py">paris = {
    # 设置手骨架
    # 设置节点的连接顺序
    # (x, y): x远离中心点，y靠近中心点
    'piano_finger': (
        (17, 0), (13, 0), (9, 0), (5, 0), (1, 0),
        (18, 17), (14, 13), (10, 9), (6, 5), (2, 1),
        (19, 18), (15, 14), (11, 10), (7, 6), (3, 2),
        (20, 19), (16, 15), (12, 11), (8, 7), (4, 3),
        (22, 21), (26, 21), (30, 21), (34, 21), (38, 21),
        (23, 22), (27, 26), (31, 30), (35, 34), (39, 38),
        (24, 23), (28, 27), (32, 31), (36, 35), (40, 39),
        (25, 24), (29, 28), (33, 32), (37, 36), (41, 40)
    ),
}

datasets = {
    'piano_finger',
}

</code></pre>
<hr>
<h3 id="网络配置">网络配置</h3>
<p><code>graph/ntu_rgb_d.py</code></p>
<pre><code class="language-py"># 设置手骨架
# 设置节点的连接顺序
# (x, y): x远离中心点，y靠近中心点

num_node = 42
self_link = [(i, i) for i in range(num_node)]

inward = [(17, 0), (13, 0), (9, 0), (5, 0), (1, 0),
          (18, 17), (14, 13), (10, 9), (6, 5), (2, 1),
          (19, 18), (15, 14), (11, 10), (7, 6), (3, 2),
          (20, 19), (16, 15), (12, 11), (8, 7), (4, 3),
          (22, 21), (26, 21), (30, 21), (34, 21), (38, 21),
          (23, 22), (27, 26), (31, 30), (35, 34), (39, 38), 
          (24, 23), (28, 27), (32, 31), (36, 35), (40, 39), 
          (25, 24), (29, 28), (33, 32), (37, 36), (41, 40)]
</code></pre>
<p>手骨架图如下所示</p>
<figure data-type="image" tabindex="3"><img src="https://s2.loli.net/2022/07/14/K1IBcUw24DMH5YX.png" alt="手骨架.png" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="https://s2.loli.net/2022/07/14/8DciE5G1TM9xv6Q.png" alt="hand-简笔画.png" loading="lazy"></figure>
<hr>
<h3 id="网络模型定义">网络模型定义</h3>
<p><code>model/agcn.py</code></p>
<pre><code class="language-py">class Model(nn.Module):
    # ----------------------------------------------------------------
    # 原代码
    # def __init__(self, 
    #             num_class=60, 
    #             num_point=25, 
    #             num_person=2, 
    #             graph=None, 
    #             graph_args=dict(), 
    #             in_channels=3):
    # ----------------------------------------------------------------
    def __init__(self, 
                num_class=10, 
                num_point=42, 
                num_person=1, 
                graph=None, 
                graph_args=dict(), 
                in_channels=3):

    # 最后面部分
    sigmoid = nn.Sigmoid()      # BCEloss需要将数据约束在0~1范围内
    return sigmoid(self.fc(x))
</code></pre>
<hr>
<h3 id="主文件">主文件</h3>
<p><code>main.py</code></p>
<pre><code class="language-py"># ----------------------------------------------------------------
# 网络训练

def train(self, epoch, save_model=False):
    self.model.train()
    self.print_log('Training epoch: {}'.format(epoch + 1))
    loader = self.data_loader['train']
    self.adjust_learning_rate(epoch)
    # for name, param in self.model.named_parameters():
    #     self.train_writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)
    loss_value = []
    self.train_writer.add_scalar('epoch', epoch, self.global_step)
    self.record_time()
    timer = dict(dataloader=0.001, model=0.001, statistics=0.001)
    process = tqdm(loader)
    if self.arg.only_train_part:
        if epoch &gt; self.arg.only_train_epoch:
            print('only train part, require grad')
            for key, value in self.model.named_parameters():
                if 'PA' in key:
                    value.requires_grad = True
                    # print(key + '-require grad')
        else:
            print('only train part, do not require grad')
            for key, value in self.model.named_parameters():
                if 'PA' in key:
                    value.requires_grad = False
                    # print(key + '-not require grad')
    for batch_idx, (data, label, index) in enumerate(process):
        self.global_step += 1
        # get data
        data = Variable(data.float().cuda(
            self.output_device), requires_grad=False)

        # --------------------------------------------------------
        # label转为tensor(batch_size, 10)
        # 很奇怪，不知道为什么，label的维度是(10, batch_size)，而且是10个batch_size长度的一维tensor
        l = torch.zeros(size=[len(label[0]), len(label)])
        for i in range(len(label[0])):
            for j in range(len(label)):
                l[i][j] = label[j][i]
        label = l

        # ----------------------------------------------------------

        label = Variable(label.float().cuda(
            self.output_device), requires_grad=False)
        # label = label.T
        # label = Variable(label.long().cuda(self.output_device), requires_grad=False)
        timer['dataloader'] += self.split_time()

        # forward
        output = self.model(data)

        # if batch_idx == 0 and epoch == 0:
        #     self.train_writer.add_graph(self.model, output)
        if isinstance(output, tuple):
            output, l1 = output
            l1 = l1.mean()
        else:
            l1 = 0
        # print(output.size())
        # print(label.size())
        loss = self.loss(output, label) + l1

        # backward
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        loss_value.append(loss.data.item())
        timer['model'] += self.split_time()

        # ----------------------------------------------------------
        # 计算准确率
        # 原代码
        # value, predict_label = torch.max(output.data, 1)
        # acc = torch.mean((predict_label == label.data).float())
        # ----------------------------------------------------------
        acc = 0

        # ----------------------------------------------------------

        self.train_writer.add_scalar('acc', acc, self.global_step)
        self.train_writer.add_scalar(
            'loss', loss.data.item(), self.global_step)
        self.train_writer.add_scalar('loss_l1', l1, self.global_step)
        # self.train_writer.add_scalar('batch_time', process.iterable.last_duration, self.global_step)

        # statistics
        self.lr = self.optimizer.param_groups[0]['lr']
        self.train_writer.add_scalar('lr', self.lr, self.global_step)
        # if self.global_step % self.arg.log_interval == 0:
        #     self.print_log(
        #         '\tBatch({}/{}) done. Loss: {:.4f}  lr:{:.6f}'.format(
        #             batch_idx, len(loader), loss.data[0], lr))
        timer['statistics'] += self.split_time()

    # statistics of time consumption and loss
    proportion = {
        k: '{:02d}%'.format(int(round(v * 100 / sum(timer.values()))))
        for k, v in timer.items()
    }
    self.print_log(
        '\tMean training loss: {:.4f}.'.format(np.mean(loss_value)))
    self.print_log(
        '\tTime consumption: [Data]{dataloader}, [Network]{model}'.format(
            **proportion))

    if save_model:
        state_dict = self.model.state_dict()
        weights = OrderedDict([[k.split('module.')[-1],
                                v.cpu()] for k, v in state_dict.items()])

        torch.save(weights, self.arg.model_saved_name + '-' +
                    str(epoch) + '-' + str(int(self.global_step)) + '.pt')

# ------------------------------------------------------------------
# 网络测试

def eval(self, epoch, save_score=False, 
        loader_name=['test'], 
        wrong_file=None, 
        result_file=None):
    if wrong_file is not None:
        f_w = open(wrong_file, 'w')
    if result_file is not None:
        f_r = open(result_file, 'w')
    self.model.eval()
    self.print_log('Eval epoch: {}'.format(epoch + 1))
    for ln in loader_name:
        loss_value = []
        score_frag = []
        right_num_total = 0
        total_num = 0
        loss_total = 0
        step = 0
        # ----------------------------------------------------------
        # 计算F1
        TP = 0
        FP = 0
        TN = 0
        FN = 0
        # ----------------------------------------------------------
        process = tqdm(self.data_loader[ln])
        for batch_idx, (data, label, index) in enumerate(process):
            with torch.no_grad():
                data = Variable(
                    data.float().cuda(self.output_device),
                    requires_grad=False)

                l = torch.zeros(size=[len(label[0]), len(label)])
                for i in range(len(label[0])):
                    for j in range(len(label)):
                        l[i][j] = label[j][i]
                label = l

                label = Variable(
                    label.float().cuda(self.output_device),
                    requires_grad=False)
                output = self.model(data)
                if isinstance(output, tuple):
                    output, l1 = output
                    l1 = l1.mean()
                else:
                    l1 = 0
                loss = self.loss(output, label)
                score_frag.append(output.data.cpu().numpy())
                loss_value.append(loss.data.item())

                # _, predict_label = torch.max(output.data, 1)
                predict_label = output.data.ge(0.5)
                step += 1

                # t = torch.ones(label.size()).cpu()

                # print('label-tf: ', label.cpu().eq(t))

                #--------------------------------------------------------------
                # 计算F1
                label = label.ge(0.5)

                TP = torch.nonzero(label &amp; predict_label == True).size(0)
                FP = torch.nonzero(~label &amp; predict_label == True).size(0)
                TN = torch.nonzero(~label &amp; ~predict_label == True).size(0)
                FN = torch.nonzero(label &amp; ~predict_label == True).size(0)

                #--------------------------------------------------------------

            # if wrong_file is not None or result_file is not None:
            #     predict = list(predict_label.cpu().numpy())
            #     true = list(label.data.cpu().numpy())
            #     for i, x in enumerate(predict):
            #         if result_file is not None:
            #             f_r.write(str(x) + ',' + str(true[i]) + '\n')
            #         if x != true[i] and wrong_file is not None:
            #             f_w.write(str(index[i]) + ',' +
            #                       str(x) + ',' + str(true[i]) + '\n')

        #-------------------------------------------------------------
        # 计算F1
        accuracy = (TP+TN)/(TP+FP+TN+FN+1)
        precision = (TP)/(TP+FP+1)
        recall = (TP)/(TP+FN+1)
        F1 = 2*precision*recall/(precision+recall+1)

        print('\naccuracy:\t', accuracy)
        print('precision:\t', precision)
        print('recall:\t\t', recall)
        print('F1:\t\t', F1, '\n')
        #-------------------------------------------------------------

        score = np.concatenate(score_frag)
        # loss = np.mean(loss_value)
        # accuracy = self.data_loader[ln].dataset.top_k(score, 1)
        if accuracy &gt; self.best_acc:
            self.best_acc = accuracy
        # # self.lr_scheduler.step(loss)
        # print('Accuracy: ', accuracy, ' model: ', self.arg.model_saved_name)
        # if self.arg.phase == 'train':
        #     self.val_writer.add_scalar('loss', loss, self.global_step)
        #     self.val_writer.add_scalar('loss_l1', l1, self.global_step)
        #     self.val_writer.add_scalar('acc', accuracy, self.global_step)

        score_dict = dict(
            zip(self.data_loader[ln].dataset.sample_name, score))
        self.print_log('\tMean {} loss of {} batches: {}.'.format(
            ln, len(self.data_loader[ln]), np.mean(loss_value)))
        for k in self.arg.show_topk:
            self.print_log('\tTop{}: {:.2f}%'.format(
                k, 100 * self.data_loader[ln].dataset.top_k(score, k)))

        if save_score:
            with open('{}/epoch{}_{}_score.pkl'.format(
                    self.arg.work_dir, epoch + 1, ln), 'wb') as f:
                pickle.dump(score_dict, f)
</code></pre>
<hr>
<h2 id="使用顺序">使用顺序</h2>
<ul>
<li>将关节数据生成为骨骼数据</li>
</ul>
<pre><code class="language-sh">python data_gen/gen_bone_data.py
</code></pre>
<ul>
<li>分别将关节和骨骼的时空数据送入 J-stream 和 B-stream，训练</li>
</ul>
<pre><code class="language-sh">python main.py --config ./config/nturgbd-cross-view/train_joint.yaml
python main.py --config ./config/nturgbd-cross-view/train_bone.yaml
</code></pre>
<ul>
<li>测试，产生各自 softmax 分数</li>
</ul>
<pre><code class="language-sh">python main.py --config ./config/nturgbd-cross-view/test_joint.yaml
python main.py --config ./config/nturgbd-cross-view/test_bone.yaml
</code></pre>
<ul>
<li>两个 softmax 分数相加 to obtain the fused score and predictthe action label</li>
</ul>
<pre><code class="language-sh">python ensemble.py --datasets piano_finger
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[图卷积学习之路-1（ST-GCN）]]></title>
        <id>https://akasaka47.github.io/post/gcn-xue-xi-zhi-lu-1-st-gcn/</id>
        <link href="https://akasaka47.github.io/post/gcn-xue-xi-zhi-lu-1-st-gcn/">
        </link>
        <updated>2022-07-13T10:34:39.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#st-gcn-2018">ST-GCN 2018</a>
<ul>
<li><a href="#%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5">邻接矩阵</a></li>
<li><a href="#%E7%A9%BA%E9%97%B4-%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5">空间-分区策略</a></li>
<li><a href="#%E6%97%B6%E9%97%B4-%E5%85%B3%E8%8A%82%E5%8D%B7%E7%A7%AF">时间-关节卷积</a></li>
<li><a href="#%E7%BD%91%E7%BB%9C%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE">网络输入数据</a></li>
<li><a href="#%E9%92%A2%E7%90%B4%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F">钢琴数据集格式</a></li>
<li><a href="#st-gcn%E7%9A%84%E7%BC%BA%E7%82%B9">ST-GCN的缺点</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="st-gcn-2018">ST-GCN 2018</h1>
<blockquote>
<p>Spatial Temporal Graph Convolutional Networks for Skeleton Based Action Recognition<br>
AAAI 2018</p>
</blockquote>
<p><strong>基于时空图卷积的动作识别</strong></p>
<p><a href="https://arxiv.org/abs/1801.07455">论文地址</a><br>
<a href="https://github.com/yysijie/st-gcn">源码地址</a><br>
<a href="https://blog.csdn.net/qq_42599237/article/details/111566607?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-4-111566607-blog-117884471.pc_relevant_multi_platform_whitelistv2&amp;spm=1001.2101.3001.4242.3&amp;utm_relevant_index=6">论文解读</a></p>
<figure data-type="image" tabindex="1"><img src="https://s2.loli.net/2022/07/13/4NgVZyzHwa8ftUG.png" alt="stgcn_1.png" loading="lazy"></figure>
<hr>
<h2 id="邻接矩阵">邻接矩阵</h2>
<p>表示图内各节点之间的连接关系</p>
<p>例：无向图内有3个节点，邻接矩阵如下所示</p>
<pre><code>0   1   1
1   0   0
1   0   0
</code></pre>
<p>表示1和2、3节点有连接</p>
<p>图内节点数为N，则邻接矩阵维度为N*N<br>
通过调整邻接矩阵元素的值，可以控制节点是否连接，以及节点连接的权重</p>
<hr>
<h2 id="空间-分区策略">空间-分区策略</h2>
<figure data-type="image" tabindex="2"><img src="https://s2.loli.net/2022/07/13/fnOBltpREDC7ySh.png" alt="stgcn_2.png" loading="lazy"></figure>
<ol>
<li>简单分区：所有节点都为一个索引，上图 ( b )，绿色全部为同一组节点；</li>
<li>距离分区：通过不同距离，将一个图（如人体骨骼）按照与目标节点的距离分为几部分，上图 ( c ），绿色是距离为0的节点，也就是节点本身，蓝色是距离为1的邻居节点，也就是与其直接相连的节点；</li>
<li>空间配置分区：通过对于目标节点而言近心端和远心端的节点进行分区，上图 ( d )，绿色是节点本身，蓝色是比原节点更接近重心的邻居节点集，黄色是远离重心的邻居节点集。</li>
</ol>
<p>ST-GCN同时使用了3种策略来学习网络参数</p>
<hr>
<h2 id="时间-关节卷积">时间-关节卷积</h2>
<figure data-type="image" tabindex="3"><img src="https://s2.loli.net/2022/07/13/v2eHJZapAUEtPkQ.png" alt="stgcn_3.png" loading="lazy"></figure>
<p>在连续帧中找到相同节点，并将其连成时域信息</p>
<p>对每一帧，选择前后各2帧，共计5帧骨架，对同一节点在5帧上的位置进行学习</p>
<hr>
<h2 id="网络输入数据">网络输入数据</h2>
<p>整个网络的输入是一个<code>(N = batch_size, C = 3 ,T = 300, V = 18, M = 2)</code>的tensor</p>
<ul>
<li>N = 64 batch_size</li>
<li>C = 3 (X,Y,S) 代表一个点的信息（X坐标、Y坐标、置信度）</li>
<li>T = 300 一个视频的帧数paper规定是300帧，不足的重头循环，多的clip</li>
<li>V = 18 根据不同的skeleton获得的节点数而定，coco是18个节点</li>
<li>M = 2 人数，paper中将人数限定在最大2个人</li>
</ul>
<hr>
<h2 id="钢琴数据集格式">钢琴数据集格式</h2>
<ul>
<li><code>data_numpy: (8, 3, 5, 42, 1)</code>  5张骨架图，对应时间维度；一只手有21个骨架点，2只手共计42个点</li>
<li><code>rgb_data: (8, 5, 3, 160, 672)</code>  3为RGB3通道；160*672为图片统一resize后的维度</li>
<li><code>multiLabel: (8, 88)</code> 88个钢琴按键</li>
<li><code>finger_label: (8, 10)</code> 10个手指</li>
</ul>
<hr>
<h2 id="st-gcn的缺点">ST-GCN的缺点</h2>
<ul>
<li>ST-GCN的注意力机制灵活性不够，掩码是与邻接矩阵直接相乘
<ul>
<li>这里说的相乘是按元素相乘，并不是矩阵相乘。</li>
<li>这就造成一个现象，就是如果邻接矩阵里面部分元素为0，那么无论对应元素为何值，最后结果都是0。</li>
<li>换句话说就是不会创造不存在的连接，比如对于“行走”动作，手和腿的联系很大，但是手和腿没有直接相连，所以效果不好。</li>
</ul>
</li>
<li>ST-GCN的第二个缺点就是没有利用骨骼数据的第二特征，
<ul>
<li>这里第一特征就是关节坐标，第二特征就是骨骼的长度和方向。</li>
<li>直觉上，第二特征也包含了丰富的行为信息。</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ant Design Pro 修改]]></title>
        <id>https://akasaka47.github.io/post/ant-design-pro-mo-gai/</id>
        <link href="https://akasaka47.github.io/post/ant-design-pro-mo-gai/">
        </link>
        <updated>2022-05-28T09:51:20.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E9%A1%B5%E8%84%9A%E9%85%8D%E7%BD%AE">页脚配置</a></li>
<li><a href="#%E7%99%BB%E5%BD%95%E7%95%8C%E9%9D%A2%E9%85%8D%E7%BD%AE">登录界面配置</a>
<ul>
<li><a href="#%E6%A0%87%E9%A2%98%E6%9B%BF%E6%8D%A2">标题替换</a></li>
<li><a href="#%E6%A0%87%E9%A2%98%E4%B8%8B%E6%96%B9%E6%96%87%E5%AD%97%E8%AF%B4%E6%98%8E">标题下方文字说明</a></li>
<li><a href="#%E6%B6%88%E9%99%A4%E5%85%B6%E4%BB%96%E7%99%BB%E5%BD%95%E9%80%89%E9%A1%B9">消除其他登录选项</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h2 id="页脚配置">页脚配置</h2>
<pre><code>/*
 * @FilePath: \myapp\src\components\Footer\index.tsx
*/

import { GithubOutlined } from '@ant-design/icons';
import { DefaultFooter } from '@ant-design/pro-layout';

const Footer: React.FC = () =&gt; {
  return (
    &lt;DefaultFooter
      copyright=&quot;@[项目名]&quot;
      links={[
        {
          key: '[key 0]',
          title: '[title 0]',
          href: 'https://akasaka47.github.io/',
          blankTarget: true,
        },
        {
          key: 'github',
          title: &lt;GithubOutlined /&gt;,
          href: '[github_link]',
          blankTarget: true,
        },
        {
          key: 'Ant Design Pro',
          title: 'Ant Design Pro',
          href: 'https://pro.ant.design',
          blankTarget: true,
        },
      ]}
    /&gt;
  );
};

export default Footer;

</code></pre>
<h2 id="登录界面配置">登录界面配置</h2>
<h3 id="标题替换">标题替换</h3>
<p><code>return - loginform - title</code></p>
<h3 id="标题下方文字说明">标题下方文字说明</h3>
<p>由<code>subTitle</code>控制，由于适配国际化，需要去对应语言区修改<br>
使用全局搜索<code>pages.layouts.userLayout.title</code>即可</p>
<h3 id="消除其他登录选项">消除其他登录选项</h3>
<p>注释以下代码</p>
<pre><code>actions={[
  &lt;FormattedMessage
    key=&quot;loginWith&quot;
    id=&quot;pages.login.loginWith&quot;
    defaultMessage=&quot;其他登录方式&quot;
  /&gt;,
  &lt;AlipayCircleOutlined key=&quot;AlipayCircleOutlined&quot; className={styles.icon} /&gt;,
  &lt;TaobaoCircleOutlined key=&quot;TaobaoCircleOutlined&quot; className={styles.icon} /&gt;,
  &lt;WeiboCircleOutlined key=&quot;WeiboCircleOutlined&quot; className={styles.icon} /&gt;,
]}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sublime Text 配置]]></title>
        <id>https://akasaka47.github.io/post/sublime-text-pei-zhi/</id>
        <link href="https://akasaka47.github.io/post/sublime-text-pei-zhi/">
        </link>
        <updated>2022-05-26T13:14:52.000Z</updated>
        <content type="html"><![CDATA[<h2 id="汉化">汉化</h2>
<ol>
<li>Ctrl+Shift+P：Package Control</li>
<li>搜索框里输入Install Package</li>
<li>搜索chinese</li>
</ol>
<h2 id="utf-8支持">UTF-8支持</h2>
<ol>
<li>Package Control</li>
<li>搜索convert</li>
</ol>
<h2 id="更换字体">更换字体</h2>
<ol>
<li>首选项-设置</li>
<li>添加<code>&quot;font_face&quot;: &quot;JetBrains Mono&quot;,</code></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[MiniConda 基本配置]]></title>
        <id>https://akasaka47.github.io/post/miniconda-ji-ben-pei-zhi/</id>
        <link href="https://akasaka47.github.io/post/miniconda-ji-ben-pei-zhi/">
        </link>
        <updated>2022-05-24T08:10:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="官方网站">官方网站</h2>
<p><a href="https://docs.conda.io/en/latest/miniconda.html">miniconda下载地址</a></p>
<h2 id="准备">准备</h2>
<p>更换源</p>
<pre><code># 清华源
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/

# 北京外国语大学
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/r
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/msys2
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud

# 源生效
conda config --set show_channel_urls yes

# 查看已有源
conda config --get channels
</code></pre>
<h2 id="环境">环境</h2>
<p>查看环境</p>
<pre><code>conda env list
</code></pre>
<p>创建环境</p>
<pre><code>conda create -n [环境名] python=[版本号]
</code></pre>
<p>删除环境</p>
<pre><code>conda remove -n [环境名] --all
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ant Design Pro 入门]]></title>
        <id>https://akasaka47.github.io/post/ant-design-pro-ru-men/</id>
        <link href="https://akasaka47.github.io/post/ant-design-pro-ru-men/">
        </link>
        <updated>2022-05-22T10:15:52.000Z</updated>
        <content type="html"><![CDATA[<h2 id="官方网站">官方网站</h2>
<p><a href="https://pro.ant.design/zh-CN/docs/overview">官方站点</a></p>
<p><a href="https://github.com/ant-design/ant-design-pro">Github Page</a></p>
<p><a href="https://umijs.org/zh-CN/docs/getting-started">UmiJS</a></p>
<h2 id="准备环境">准备环境</h2>
<ol>
<li>
<p>安装<strong>Node.js</strong></p>
</li>
<li>
<p>npm更换为国内镜像</p>
</li>
</ol>
<pre><code class="language-bash">npm config set registry https://registry.npm.taobao.org
</code></pre>
<h2 id="从源码安装">从源码安装</h2>
<ol>
<li>从官网下载源码</li>
<li>解压源码，进入目录，打开命令行</li>
</ol>
<pre><code class="language-bash">npm install
npm run start
</code></pre>
<h2 id="使用yarn安装">使用<code>yarn</code>安装</h2>
<pre><code class="language-bash">yarn create umi myapp
# 选择 ant-design-pro
# 选择 TypeScript
# 选择 simple
cd myapp
tyarn
# 开始运行
yarn run start
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang使用命令行]]></title>
        <id>https://akasaka47.github.io/post/golang-shi-yong-ming-ling-xing/</id>
        <link href="https://akasaka47.github.io/post/golang-shi-yong-ming-ling-xing/">
        </link>
        <updated>2022-05-20T00:53:17.000Z</updated>
        <content type="html"><![CDATA[<h2 id="cmd结构体">cmd结构体</h2>
<pre><code>type Cmd struct {  
    Path         string　		//运行命令的路径，绝对路径或者相对路径  
    Args         []string　		// 命令参数  
    Env          []string  		//进程环境，如果环境为空，则使用当前进程的环境  
    Dir          string　　　		//指定command的工作目录，如果dir为空，则comman在调用进程所在当前目录中运行  
    Stdin        io.Reader　　//标准输入，如果stdin是nil的话，进程从null device中读取（os.DevNull），stdin也可以时一个文件，否则的话则在运行过程中再开一个goroutine去  
　　　　　　　　　　　　　//读取标准输入  
    Stdout       io.Writer       //标准输出  
    Stderr       io.Writer　　//错误输出，如果这两个（Stdout和Stderr）为空的话，则command运行时将响应的文件描述符连接到os.DevNull  
    ExtraFiles   []*os.File 　　  
    SysProcAttr  *syscall.SysProcAttr  
    Process      *os.Process    //Process是底层进程，只启动一次  
    ProcessState *os.ProcessState　　//ProcessState包含一个退出进程的信息，当进程调用Wait或者Run时便会产生该信息．  
}  
</code></pre>
<h2 id="用法一直接在当前目录使用并返回结果">用法一：直接在当前目录使用并返回结果</h2>
<pre><code>func Cmd(commandName string, params []string) (string, error) {
    cmd := exec.Command(commandName, params...)
    fmt.Println(&quot;Cmd&quot;, cmd.Args)
    var out bytes.Buffer
    cmd.Stdout = &amp;out
    cmd.Stderr = os.Stderr
    err := cmd.Start()
    if err != nil {
        return &quot;&quot;, err
    }
    err = cmd.Wait()
    return out.String(), err
}
</code></pre>
<h2 id="用法二在命令位置使用并返回结果">用法二：在命令位置使用并返回结果</h2>
<pre><code>func CmdAndChangeDir(dir string, commandName string, params []string) (string, error) {
    cmd := exec.Command(commandName, params...)
    fmt.Println(&quot;CmdAndChangeDir&quot;, dir, cmd.Args)
    var out bytes.Buffer
    cmd.Stdout = &amp;out
    cmd.Stderr = os.Stderr
    cmd.Dir = dir
    err := cmd.Start()
    if err != nil {
        return &quot;&quot;, err
    }
    err = cmd.Wait()
    return out.String(), err
}
</code></pre>
<h2 id="用法三在命令位置使用并实时输出每行结果">用法三：在命令位置使用并实时输出每行结果</h2>
<pre><code>func CmdAndChangeDirToShow(dir string, commandName string, params []string) error {
  cmd := exec.Command(commandName, params...)
  fmt.Println(&quot;CmdAndChangeDirToFile&quot;, dir, cmd.Args)
  //StdoutPipe方法返回一个在命令Start后与命令标准输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。
  stdout, err := cmd.StdoutPipe()
  if err != nil {
    fmt.Println(&quot;cmd.StdoutPipe: &quot;, err)
    return err
  }
  cmd.Stderr = os.Stderr
  cmd.Dir = dir
  err = cmd.Start()
  if err != nil {
    return err
  }
  //创建一个流来读取管道内内容，这里逻辑是通过一行一行的读取的
  reader := bufio.NewReader(stdout)
  //实时循环读取输出流中的一行内容
  for {
    line, err2 := reader.ReadString('\n')
    if err2 != nil || io.EOF == err2 {
      break
    }
    fmt.Println(line)
  }
  err = cmd.Wait()
  return err
}
</code></pre>
<h2 id="用法四在命令位置使用并实时写入每行结果到文件">用法四：在命令位置使用并实时写入每行结果到文件</h2>
<pre><code>func CmdAndChangeDirToFile(fileName, dir, commandName string, params []string) error {
    Path, err := GetCurrentPath()
    if err != nil {
        return err
    }
    Path += &quot;logs/cmdDir/&quot;
    EnsureDir(Path)
    f, err := os.Create(Path + fileName) //创建文件
    defer f.Close()
    cmd := exec.Command(commandName, params...)
    fmt.Println(&quot;CmdAndChangeDirToFile&quot;, dir, cmd.Args)
    //StdoutPipe方法返回一个在命令Start后与命令标准输出关联的管道。Wait方法获知命令结束后会关闭这个管道，一般不需要显式的关闭该管道。
    stdout, err := cmd.StdoutPipe()
    if err != nil {
        fmt.Println(&quot;cmd.StdoutPipe: &quot;, err)
        return err
    }
    cmd.Stderr = os.Stderr
    cmd.Dir = dir
    err = cmd.Start()
    if err != nil {
        return err
    }
    //创建一个流来读取管道内内容，这里逻辑是通过一行一行的读取的
    reader := bufio.NewReader(stdout)
    //实时循环读取输出流中的一行内容
    for {
        line, err2 := reader.ReadString('\n')
        if err2 != nil || io.EOF == err2 {
            break
        }
        _, err = f.WriteString(line) //写入文件(字节数组)
        f.Sync()
    }
    _, err = f.WriteString(&quot;=================处理完毕========================&quot;) //写入文件(字节数组)
    f.Sync()
    err = cmd.Wait()
    return err
}
</code></pre>
<h2 id="用法五bash-c-方式执行">用法五：bash -c 方式执行</h2>
<pre><code>// 后台运行一个命令 bash -c 方式
func CmdBash(commandName string) *exec.Cmd {
	cmd := exec.Command(&quot;/bin/bash&quot;, &quot;-c&quot;, commandName)
	fmt.Println(line)(cmd.Args)
 
	go func() {
		var out bytes.Buffer
		cmd.Stdout = &amp;out
		cmd.Stderr = os.Stderr
		_ = cmd.Run()
	}()
 
	return cmd
}
</code></pre>
<hr>
<p>转自CSDN：<a href="https://blog.csdn.net/whatday/article/details/109277998">原文链接</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang操作Redis]]></title>
        <id>https://akasaka47.github.io/post/golang-cao-zuo-redis/</id>
        <link href="https://akasaka47.github.io/post/golang-cao-zuo-redis/">
        </link>
        <updated>2022-05-19T12:27:05.000Z</updated>
        <content type="html"><![CDATA[<h2 id="导入对应包">导入对应包</h2>
<pre><code>import &quot;github.com/go-redis/redis&quot;

</code></pre>
<h2 id="连接到数据库">连接到数据库</h2>
<pre><code>var redisDB *redis.Client

func initRedis() (err error) {
	redisDB = redis.NewClient(&amp;redis.Options{
		Addr:     &quot;127.0.0.1:6379&quot;, // 指定
		Password: &quot;&quot;,
		DB:       0, // redis一共16个库，指定其中一个库即可
	})
	_, err = redisDB.Ping().Result()
	if err != nil {
		return err
	}
	fmt.Println(&quot;Connected to Redis!&quot;)
	return
}

</code></pre>
<h2 id="常用操作">常用操作</h2>
<pre><code>redisDB.Set(key, value)		          //设置键值对
redisDB.Get(key)			            //获取键值对
redisDB.Expire(key, time)	        //设置过期时间
redisDB.RPush(key, values)	      //设置列表
val, err := redisDB.LRange(key, 0, -1).Result()	//获取列表中所有元素

</code></pre>
]]></content>
    </entry>
</feed>